<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [], "layout": [[0.0, 1.0], [0.25, 1.0], [0.5, 1.0], [0.75, 1.0], [1.0, 1.0]], "plan": ["Here is a detailed implementation plan for a winning solution to the House\nPrices prediction competition.  ## 1. Approach: Overall Strategy and Model\nArchitecture  Our goal is to build a highly accurate and robust predictive\nmodel. Given the tabular nature of the data and the RMSE evaluation metric, an\nensemble of diverse, powerful models is the most effective strategy. We will\nimplement a two-level stacking ensemble.  ### 1.1. High-Level Architecture  The\narchitecture consists of two layers of models:  1.  **Level 0 (Base Models):** A\ndiverse set of individual models will be trained on the preprocessed data. Their\npredictions will serve as input features for the next level. We will use a\nvariety of gradient boosting models and a regularized linear model to ensure\ndiversity. 2.  **Level 1 (Meta-Model):** A single, simpler model will be trained\non the predictions generated by the Level 0 models. This meta-model learns the\noptimal way to combine the base model predictions to produce the final, more\naccurate forecast.  ### 1.2. Workflow  The end-to-end process will be as\nfollows:  1.  **Data Preprocessing:** Load, clean, and engineer features from\nthe raw `train.csv` and `test.csv` files. 2.  **Cross-Validation Setup:** A\nK-Fold cross-validation strategy will be used to train the Level 0 models,\ngenerate out-of-fold (OOF) predictions for training the meta-model, and ensure\nour validation score is robust. 3.  **Level 0 Model Training:** Within each fold\nof the cross-validation:     *   Train each base model (LightGBM, XGBoost,\nCatBoost, Ridge).     *   Generate predictions on the validation set (to create\nthe OOF training data for Level 1).     *   Generate predictions on the entire\ntest set. 4.  **Level 1 Model Training:**     *   Train the meta-model using the\nOOF predictions from Level 0 as input features and the original target values as\nthe label.     *   Use the trained meta-model to make final predictions on the\ntest set predictions generated by the Level 0 models. 5.  **Submission:** Format\nthe final predictions into the required submission file format.  This stacked\ngeneralization approach leverages the strengths of different algorithms, often\nleading to performance that surpasses any single model.  ---  ## 2. Data\nProcessing  This is the most critical phase. Careful cleaning, imputation, and\nfeature engineering will provide the foundation for our models. We will use\n`pandas` and `numpy` for data manipulation and `scikit-learn` for preprocessing\nsteps.  ### 2.1. Initial Setup  1.  **Load Data:** Load `train.csv` and\n`test.csv` into pandas DataFrames. 2.  **Combine Datasets:** Concatenate the\ntrain and test sets (excluding the `SalePrice` and `Id` columns) to ensure\nconsistent processing for all features. Store `train_ID` and `test_ID` for the\nfinal submission. 3.  **Target Transformation:** The `SalePrice` distribution is\nright-skewed. Apply a `log1p` transformation (`numpy.log1p`) to the target\nvariable to make it more Gaussian. This stabilizes variance and improves the\nperformance of most models. All predictions will be made on this log-transformed\nscale, and the final result will be converted back using `numpy.expm1`.  ###\n2.2. Outlier Handling  Based on common analyses of this dataset, there are a few\ninfluential outliers in the training data that can harm model performance. *\nVisualize `GrLivArea` vs. `log1p(SalePrice)`. *   Remove the two training data\npoints where `GrLivArea > 4000` and `SalePrice` is low, as they deviate\nsignificantly from the main trend.  ### 2.3. Missing Value Imputation  Impute\nmissing values based on the context provided in `data_description.txt`.  *\n**Categorical `NaN` meaning \"None\"**: For the following features, `NaN`\nsignifies the absence of the feature. Impute these with the string `\"None\"`.\n*   `PoolQC`, `MiscFeature`, `Alley`, `Fence`, `FireplaceQu`, `GarageType`,\n`GarageFinish`, `GarageQual`, `GarageCond`, `BsmtQual`, `BsmtCond`,\n`BsmtExposure`, `BsmtFinType1`, `BsmtFinType2`, `MasVnrType`. *   **Numerical\n`NaN` meaning 0**: For these features, `NaN` also implies absence. Impute with\n`0`.     *   `GarageYrBlt`, `GarageArea`, `GarageCars`, `BsmtFinSF1`,\n`BsmtFinSF2`, `BsmtUnfSF`, `TotalBsmtSF`, `BsmtFullBath`, `BsmtHalfBath`,\n`MasVnrArea`. *   **Mode Imputation**: For these categorical features with a\nsmall number of missing values, impute with the mode (most frequent value) of\nthe column.     *   `MSZoning`, `Electrical`, `KitchenQual`, `Exterior1st`,\n`Exterior2nd`, `SaleType`. *   **Special Cases**:     *   `LotFrontage`: This is\nlikely related to the neighborhood. Impute missing values with the median\n`LotFrontage` of their respective `Neighborhood`.     *   `Functional`: The data\ndescription says `NaN` means \"Typ\" (Typical). Impute with `\"Typ\"`.     *\n`Utilities`: This feature has almost no variance. Drop this column.  ### 2.4.\nFeature Engineering  Create new, more informative features from the existing\ndata.  *   **Combine Features**:     *   `TotalSF`: `TotalBsmtSF` + `1stFlrSF` +\n`2ndFlrSF`.     *   `TotalBath`: `FullBath` + 0.5 * `HalfBath` + `BsmtFullBath`\n+ 0.5 * `BsmtHalfBath`.     *   `TotalPorchSF`: `OpenPorchSF` + `3SsnPorch` +\n`EnclosedPorch` + `ScreenPorch` + `WoodDeckSF`.     *   `HasPool`: `1` if\n`PoolArea > 0`, else `0`.     *   `Has2ndfloor`: `1` if `2ndFlrSF > 0`, else\n`0`.     *   `HasGarage`: `1` if `GarageArea > 0`, else `0`.     *   `HasBsmt`:\n`1` if `TotalBsmtSF > 0`, else `0`.     *   `HasFireplace`: `1` if `Fireplaces >\n0`, else `0`. *   **Simplify Features / Type Conversion**:     *   `MSSubClass`:\nThis is a categorical feature disguised as a number. Convert it to a string\n(`str`).     *   `OverallCond`, `YrSold`, `MoSold`: These are also categorical.\nConvert them to string type.  ### 2.5. Feature Transformation  *   **Ordinal\nFeature Mapping**: Convert categorical features with an inherent order into\nnumerical values.     *   Example mapping for quality: `{'Ex': 5, 'Gd': 4, 'TA':\n3, 'Fa': 2, 'Po': 1, 'None': 0}`.     *   Apply this logic to all features with\nan explicit quality scale (e.g., `ExterQual`, `BsmtQual`, `HeatingQC`,\n`KitchenQual`, `FireplaceQu`, `GarageQual`, `PoolQC`). *   **Skewed Feature\nCorrection**:     *   Identify numerical features with high skewness (e.g., skew\n> 0.5) using `scipy.stats.skew`.     *   Apply a Box-Cox transformation\n(`scipy.special.boxcox1p`) to these skewed features to make their distributions\nmore normal. *   **Categorical Feature Encoding**:     *   For all remaining\nnon-ordinal categorical features, use one-hot encoding via `pandas.get_dummies`.\nThis will convert them into a numerical format that all models can process.  The\noutput of this stage will be a fully-numeric, clean feature matrix `X` and a\ntransformed target vector `y_log`.  ---  ## 3. Model Selection  The choice of\nmodels is crucial for the success of the stacking ensemble. We need models that\nare individually strong but also diverse in how they learn from the data.  ###\n3.1. Level 0: Base Models  *   **LightGBM (`lightgbm.LGBMRegressor`):**     *\n**Why:** Extremely fast and memory-efficient gradient boosting framework. It\nuses a leaf-wise growth strategy, which often leads to better accuracy. A must-\nhave for any competitive solution. *   **XGBoost (`xgboost.XGBRegressor`):**\n*   **Why:** The original \"Kaggle winner.\" It is highly robust, regularized, and\nconsistently delivers top-tier performance. It provides a different approach to\ntree-building (level-wise) than LightGBM, adding diversity. *   **CatBoost\n(`catboost.CatBoostRegressor`):**     *   **Why:** A newer gradient boosting\nlibrary that excels with categorical data. Although we one-hot encode, its\ninternal symmetric tree growth and ordering-based boosting offer a unique\nlearning pattern, making it a valuable and diverse addition to the ensemble. *\n**Ridge Regression (`sklearn.linear_model.RidgeCV`):**     *   **Why:** A\nsimple, powerful linear model with L2 regularization. It will capture linear\nrelationships that tree-based models might miss. Its inclusion provides\nsignificant model diversity, which is key to a strong ensemble. It is less prone\nto overfitting than a plain linear regression.  ### 3.2. Level 1: Meta-Model  *\n**Ridge Regression (`sklearn.linear_model.RidgeCV`):**     *   **Why:** A\nsimple, regularized linear model is the ideal choice for a meta-model. It is\nrobust, fast, and effectively learns the linear combination of the base model\npredictions without overfitting them. Using `RidgeCV` allows the model to find\nthe optimal regularization strength `alpha` automatically via cross-validation.\n---  ## 4. Training Strategy  We will use a structured training pipeline to\nbuild our ensemble.  ### 4.1. Validation Framework  *   **Strategy:** K-Fold\nCross-Validation. *   **Library:** `sklearn.model_selection.KFold`. *\n**Configuration:**     *   `n_splits=10`: A higher number of splits provides a\nmore stable validation score, crucial for this small dataset.     *\n`shuffle=True`: Randomize the data before splitting.     *   `random_state=42`:\nEnsure reproducibility of the splits.  ### 4.2. Level 0 Model Training and\nPrediction  1.  Initialize empty OOF and test prediction arrays for each base\nmodel. The OOF array should have the same length as the training set, and the\ntest prediction array should be `(n_folds, n_test_samples)`. 2.  Loop through\neach fold generated by `KFold`:     *   Split the data into training and\nvalidation sets for the current fold.     *   For each base model:         *\nInstantiate the model with the specified hyperparameters.         *   Train the\nmodel on the fold's training data. Use the fold's validation set for early\nstopping to prevent overfitting.         *   Predict on the fold's validation\ndata and store these predictions in the corresponding slice of the OOF array.\n*   Predict on the full test set and store these predictions in the\ncorresponding row of the test prediction array. 3.  After the loop finishes,\naverage the test predictions across all folds. This yields one final prediction\nvector for the test set from each base model. 4.  The OOF arrays will serve as\nthe feature set (`X_meta`) for the Level 1 model. The averaged test predictions\nwill be the test set (`X_test_meta`) for the Level 1 model.  ### 4.3.\nRecommended Hyperparameters (Starting Point)  *   **LightGBM\n(`LGBMRegressor`):**     *   `objective`: `'regression_l1'`     *\n`n_estimators`: `2000`     *   `learning_rate`: `0.01`     *   `num_leaves`:\n`31`     *   `max_depth`: `-1`     *   `reg_alpha`: `0.1`     *   `reg_lambda`:\n`0.1`     *   `colsample_bytree`: `0.7`     *   `subsample`: `0.7`     *\n`n_jobs`: `-1`     *   **Early Stopping:** `callbacks=[lgb.early_stopping(100,\nverbose=False)]`  *   **XGBoost (`XGBRegressor`):**     *   `objective`:\n`'reg:squarederror'`     *   `eval_metric`: `'rmse'`     *   `n_estimators`:\n`2000`     *   `learning_rate`: `0.01`     *   `max_depth`: `4`     *\n`colsample_bytree`: `0.7`     *   `subsample`: `0.7`     *   `reg_alpha`:\n`0.005`     *   `n_jobs`: `-1`     *   `early_stopping_rounds`: `100` (set in\nconstructor)  *   **CatBoost (`CatBoostRegressor`):**     *   `iterations`:\n`3000`     *   `learning_rate`: `0.02`     *   `depth`: `6`     *\n`l2_leaf_reg`: `3`     *   `loss_function`: `'RMSE'`     *\n`early_stopping_rounds`: `100`     *   `verbose`: `False`  *   **Ridge\n(`RidgeCV`):**     *   `alphas`: `[0.1, 1.0, 10.0, 100.0]` (will be cross-\nvalidated internally)     *   `cv`: `None` (uses efficient LOO-CV by default)\n### 4.4. Level 1 Model Training  1.  Instantiate the `RidgeCV` meta-model. 2.\nTrain it on `X_meta` (the OOF predictions) and the original `y_log` target. 3.\nUse the trained model to predict on `X_test_meta` (the averaged test\npredictions). This gives the final log-scale predictions.  ---  ## 5. Evaluation\n### 5.1. Local Validation  *   **Metric:** Root Mean Squared Error (RMSE). *\n**Procedure:**     1.  After training the Level 1 meta-model, generate\npredictions on the `X_meta` (OOF) data.     2.  Inverse-transform these\npredictions using `numpy.expm1`.     3.  Inverse-transform the original `y_log`\ntarget vector to get the true `SalePrice` values.     4.  Calculate the RMSE\nusing `sklearn.metrics.mean_squared_error(y_true, y_pred, squared=False)`. *\n**Importance:** This local CV score is our primary indicator of model\nperformance and should be a reliable estimate of the leaderboard score.  ###\n5.2. Submission File Generation  1.  Take the final predictions on the test set\nfrom the Level 1 model. 2.  Inverse-transform these predictions using\n`numpy.expm1`. 3.  Create a pandas DataFrame with two columns: `Id` (using the\nstored `test_ID`) and `SalePrice` (the final predictions). 4.  Save the\nDataFrame to `submission.csv` with `index=False`.  ---  ## 6. Potential Issues\nand Mitigations  *   **Data Leakage:** Our preprocessing strategy (concatenating\ntrain/test) introduces minor leakage (e.g., when using mode imputation). For\nthis competition, this is an acceptable trade-off for simplicity. To be 100%\npure, all fitting (imputers, scalers) should be done on the training part of\neach CV fold and then applied to the validation/test sets. *   **Overfitting the\nEnsemble:** The primary defense is using a simple meta-model (`RidgeCV`) and\ngenerating clean OOF predictions via cross-validation. If overfitting is\nsuspected (large gap between CV and LB scores), we could try an even simpler\nmeta-model like `LassoCV` or simply average the base model predictions. *\n**Model/Seed Sensitivity:** The results of GBDT models can vary slightly with\ndifferent random seeds. A potential improvement would be to re-train the entire\nLevel 0 stack with 3-5 different seeds and average the final test predictions.\nThis adds robustness at the cost of computation time. *   **Hyperparameter\nOptimality:** The provided hyperparameters are strong starting points. For a\npotential performance boost, `Optuna` or `bayesian-optimization` could be used\nto tune each base model, but this should be done carefully within the cross-\nvalidation loop to avoid overfitting.", "Here is a detailed implementation plan for a winning solution to the House\nPrices prediction competition.  ## 1. Approach: Overall Strategy and Model\nArchitecture  The previous solution, a standard stacking ensemble, achieved a\nstrong result (RMSE 0.108). To surpass this, we will implement a more\nsophisticated and robust architecture that introduces greater model diversity,\ntailored data preprocessing pipelines, and automated hyperparameter tuning.  Our\nstrategy is built on three core principles: 1.  **Maximal Model Diversity:** We\nwill use a wider array of model families (tree-based, linear, neural network)\nthat learn different types of patterns from the data. 2.  **Optimized Data\nRepresentation:** We will create two distinct preprocessing pipelines, one\noptimized for tree-based models and another for linear and neural network\nmodels. This ensures each model receives data in its ideal format. 3.\n**Automated Optimization:** We will use Bayesian optimization (`Optuna`) to\nsystematically find high-performing hyperparameters for our models, moving\nbeyond well-chosen defaults.  ### 1.1. High-Level Architecture  The architecture\nis a two-level stacking ensemble with a final blending step:  1.  **Level 0\n(Base Models):** A diverse set of five models will be trained. Their out-of-fold\n(OOF) predictions will become the features for the meta-model.     *   **Group A\n(Tree-based):** LightGBM, XGBoost, CatBoost.     *   **Group B (Linear/NN):**\nElasticNet, MLP Neural Network. 2.  **Level 1 (Meta-Model):** A `LGBMRegressor`\nwill be trained on the predictions from the Level 0 models. We use a GBDT here\nto capture potential non-linear interactions between the base model predictions.\n3.  **Final Blending:** The final prediction will be a weighted average of the\nLevel 1 stacker's output and the predictions from the best-performing Level 0\nmodels. This adds a final layer of stability and often yields a small\nperformance boost.  ### 1.2. Workflow  1.  **Data Ingestion & Initial\nCleaning:** Load data, apply target transformation (`log1p`), and handle\noutliers. 2.  **Hyperparameter Optimization:** Use `Optuna` to find the optimal\nhyperparameters for each Level 0 model before the main training phase. 3.\n**Cross-Validated Training:**     *   Use a 10-Fold CV strategy.     *   Inside\neach fold:         *   Apply **Pipeline A (for Group A models)**, which includes\ntarget encoding.         *   Apply **Pipeline B (for Group B models)**, which\nincludes one-hot encoding and scaling.         *   Train all Level 0 models on\nthe appropriate data pipeline.         *   Generate OOF predictions on the\nvalidation set and predictions on the test set. 4.  **Meta-Model Training:**\nTrain the Level 1 `LGBMRegressor` on the collected OOF predictions. 5.  **Final\nPrediction & Blending:** Generate predictions from the meta-model on the test\nset predictions. Blend these with the best single model predictions to create\nthe final submission file.  ---  ## 2. Data Processing  This phase is split into\ntwo specialized pipelines to cater to the different needs of our model groups.\n### 2.1. Initial Setup (Common to Both Pipelines)  1.  **Load & Combine:** Load\n`train.csv` and `test.csv`. Concatenate them for consistent processing, storing\n`Id` columns and the `SalePrice` target separately. 2.  **Target\nTransformation:** Apply `numpy.log1p` to `SalePrice` to normalize its\ndistribution. All modeling will be done on this log-transformed target. 3.\n**Outlier Removal:** As in the previous analysis, remove the two training data\npoints with `GrLivArea > 4000` as they are well-documented influential outliers.\n4.  **Initial Imputation & Feature Creation:** Perform the same robust\nimputation as the previous design (e.g., `None` for missing `PoolQC`, median\n`LotFrontage` by `Neighborhood`, etc.). Create the core engineered features:\n`TotalSF`, `TotalBath`, `TotalPorchSF`, and boolean flags like `HasPool`,\n`HasBsmt`, etc. 5.  **Type Conversion:** Convert numerical-looking categoricals\nlike `MSSubClass` and `MoSold` to string type.  ### 2.2. Pipeline A: For Tree-\nBased Models (LGBM, XGBoost, CatBoost)  Tree models are less sensitive to\nfeature scaling but can benefit immensely from encodings that preserve or create\nmeaningful splits.  1.  **Feature Engineering:**     *   **Interaction\nFeatures:** Create high-value interaction terms, such as `OverallQual * TotalSF`\nand `GrLivArea * Neighborhood_Med_Price`. The `Neighborhood_Med_Price` feature\nwill be the median `SalePrice` of each neighborhood, calculated from the\ntraining set. 2.  **Target Encoding:**     *   For high-cardinality categorical\nfeatures (`Neighborhood`, `Exterior1st`, `Exterior2nd`), apply target encoding.\n*   **CRITICAL:** This must be done *inside* the cross-validation loop. For each\nfold, the encoder should be fit *only* on the fold's training data and then used\nto transform both the fold's training and validation sets. This prevents target\nleakage. For the test set, transform it using an encoder fit on the entire\ntraining dataset. 3.  **Categorical Encoding:**     *   For remaining, lower-\ncardinality categorical features, use standard `pandas.get_dummies`.  ### 2.3.\nPipeline B: For Linear & NN Models (ElasticNet, MLP)  These models require all\ndata to be numeric and properly scaled.  1.  **Feature Engineering:**     *\nUse the same engineered features as Pipeline A for consistency. 2.\n**Categorical Encoding:**     *   Use\n`sklearn.preprocessing.OneHotEncoder(handle_unknown='ignore')` on all\ncategorical features. This is more robust than `get_dummies` as it can handle\ncategories in the validation/test set that were not seen during training. 3.\n**Skew Correction:**     *   Identify numerical features with a skewness > 0.5.\n*   Apply a Box-Cox transformation (`scipy.special.boxcox1p`) to these features.\n4.  **Scaling:**     *   Apply `sklearn.preprocessing.RobustScaler` to all\nfeatures. This scaler is robust to outliers and centers the data, which is ideal\nfor both linear models and neural networks.  ---  ## 3. Model Selection  The\nmodel choices prioritize strength and diversity.  ### 3.1. Level 0: Base Models\n*   **LGBM/XGBoost/CatBoost (`LGBMRegressor`, `XGBRegressor`,\n`CatBoostRegressor`):**     *   **Why:** These three are the state-of-the-art\nGBDTs. While similar, their different tree-building algorithms (leaf-wise vs.\nlevel-wise vs. symmetric) and regularization techniques provide essential\ndiversity. They will be trained on Pipeline A data. *   **ElasticNet\n(`sklearn.linear_model.ElasticNetCV`):**     *   **Why:** A regularized linear\nmodel that combines L1 and L2 penalties. It captures linear trends, is robust to\ncollinear features, and provides a fundamentally different perspective than tree\nmodels. It will be trained on Pipeline B data. The `CV` version automatically\ntunes the regularization strength. *   **MLP Regressor\n(`sklearn.neural_network.MLPRegressor`):**     *   **Why:** A simple multi-layer\nperceptron. It introduces a non-linear modeling approach completely different\nfrom boosting trees. This is a powerful source of diversity. It requires the\nscaled data from Pipeline B.  ### 3.2. Level 1: Meta-Model  *   **LightGBM\n(`LGBMRegressor`):**     *   **Why:** We are elevating the meta-model from a\nsimple linear regressor to a more powerful GBDT. With only a few input features\n(the OOF predictions), a carefully configured LGBM can capture non-linear\nrelationships between the base models' strengths and weaknesses. We will use\nconservative hyperparameters to prevent overfitting.  ---  ## 4. Training\nStrategy  This is a multi-stage process involving optimization, cross-\nvalidation, and stacking.  ### 4.1. Hyperparameter Optimization with Optuna\nBefore the main CV loop, find optimal parameters for each of the five Level 0\nmodels.  1.  **Setup:** For each model, define a search space for its key\nhyperparameters. 2.  **Objective Function:** Create a function that takes an\n`optuna.Trial` object. Inside, it will:     *   Suggest hyperparameters from the\nsearch space.     *   Instantiate the model with these parameters.     *\nPerform a simple 3-fold cross-validation on the full training data.     *\nReturn the average RMSE from the 3 folds. 3.  **Execution:** Run an\n`optuna.create_study(direction='minimize')` and call `study.optimize()` with the\nobjective function for 50-100 trials per model. Store the `study.best_params`.\n### 4.2. Main Training Loop (10-Fold CV)  1.  **Initialization:** Use\n`sklearn.model_selection.KFold` with `n_splits=10`, `shuffle=True`, and a fixed\n`random_state`. Initialize empty arrays to store OOF predictions for each model\nand averaged test predictions. 2.  **Loop:** Iterate through the K-Fold splits\n(`train_idx`, `val_idx`):     *   **Data Prep:**         *   Create `X_train_A`,\n`X_val_A` using Pipeline A. **Fit target encoders on `X_train_A` only.**\n*   Create `X_train_B`, `X_val_B` using Pipeline B. **Fit one-hot encoders and\nscalers on `X_train_B` only.**     *   **Model Training & Prediction:**\n*   For each **Group A** model (LGBM, XGBoost, CatBoost):             *\nInstantiate with the pre-tuned `best_params`.             *   Train on\n`X_train_A` and `y_train`, using the validation set for early stopping.\n*   Predict on `X_val_A` and store in the OOF array.             *   Predict on\nthe full test set (transformed with Pipeline A fit on all training data) and add\nto the test prediction array.         *   For each **Group B** model\n(ElasticNet, MLP):             *   Instantiate with `best_params`.             *\nTrain on `X_train_B` and `y_train`.             *   Predict on `X_val_B` and\nstore in OOF array.             *   Predict on the full test set (transformed\nwith Pipeline B fit on all training data) and add to test predictions. 3.\n**Finalize Level 0 Predictions:** After the loop, average the test predictions\nacross the 10 folds for each model. This creates one final test prediction\nvector per model. The OOF array is now the complete training set for the meta-\nmodel.  ### 4.3. Level 1 Model Training  1.  **Prepare Data:** `X_meta_train` is\nthe OOF array (shape: `n_samples, 5`). `X_meta_test` is the set of averaged test\npredictions (shape: `n_test_samples, 5`). 2.  **Train Meta-Model:** Instantiate\n`LGBMRegressor` with conservative parameters (e.g., `n_estimators=500`,\n`learning_rate=0.01`, `num_leaves=15`). Train it on `X_meta_train` and the\noriginal `y_log` target. 3.  **Final Stacked Prediction:** Predict on\n`X_meta_test`. This is `stacked_preds`.  ---  ## 5. Evaluation & Submission  ###\n5.1. Local Validation  *   **Metric:** Root Mean Squared Error (RMSE). *\n**Procedure:** Our main validation score comes directly from the stacking\nprocess. Calculate the RMSE between the OOF predictions generated by the trained\n**Level 1 meta-model** and the original `y_log` target. This score is a robust\nestimate of leaderboard performance because no model has seen the data it is\nbeing evaluated on during its own training.  ### 5.2. Post-processing and\nSubmission  1.  **Blending:** Create the final prediction using a weighted\naverage. For example:     *   `final_preds = 0.7 * stacked_preds + 0.3 *\nbest_single_model_test_preds`     *   The \"best single model\" is identified by\nits individual OOF RMSE score. The weights (0.7, 0.3) can be tuned by observing\nthe OOF validation score. 2.  **Inverse Transformation:** Apply `numpy.expm1` to\n`final_preds` to convert them back to the original `SalePrice` scale. 3.  **File\nGeneration:** Create a `submission.csv` file with the `Id` and final transformed\n`SalePrice` predictions.  ---  ## 6. Potential Issues and Mitigations  *\n**Target Leakage:** The greatest risk is improper implementation of target\nencoding. **Mitigation:** Strictly enforce that the target encoder is fit *only*\non the training portion of each fold. *   **Computational Cost:** The\nhyperparameter optimization step is time-consuming. **Mitigation:** Start with a\nlower number of trials (e.g., 25) for `Optuna`. If necessary, skip `Optuna` and\nuse the well-tested default hyperparameters from the previous design as a strong\nbaseline. *   **Overfitting the Meta-Model:** Using `LGBMRegressor` as a meta-\nmodel is powerful but riskier than `Ridge`. **Mitigation:** Use highly\nconservative hyperparameters: a low learning rate, a small number of estimators,\nand strong L1/L2 regularization (`reg_alpha`, `reg_lambda`). Monitor the meta-\nmodel's OOF score closely. *   **Pipeline Complexity:** Managing two separate\npreprocessing pipelines adds complexity. **Mitigation:** Encapsulate each\npipeline within its own Python function or class to keep the main training loop\nclean and readable, reducing the chance of errors. *   **Reproducibility:**\nMultiple sources of randomness exist (CV splits, model initializations, Optuna\ntrials). **Mitigation:** Set a global random seed and pass it to `KFold`, all\nmodels that accept a `random_state`, and the `Optuna` sampler to ensure\nconsistent results across runs.", "Here is a detailed implementation plan for a winning solution to the House\nPrices prediction competition.  ## 1. Approach: Overall Strategy and Model\nArchitecture  The previous approach successfully used a classic stacking\nensemble. To improve upon this, we will adopt a more sophisticated, two-stage\nstrategy focused on maximizing model diversity, meticulous hyperparameter\ntuning, and a robust blending technique instead of a trainable meta-model. This\nreduces the risk of overfitting the second level and often yields more stable\nand accurate results.  ### 1.1. High-Level Architecture  The architecture is a\ntwo-stage process:  1.  **Stage 1: Diverse Model Training and Prediction\nGeneration.**     *   We will train a diverse set of powerful base models.\nCrucially, each model will be individually tuned using a rigorous hyperparameter\noptimization process integrated with our cross-validation framework.     *   For\neach model, we will generate out-of-fold (OOF) predictions for the entire\ntraining set and averaged predictions for the test set. These predictions will\nbe saved and used as inputs for the next stage.  2.  **Stage 2: Optimized\nWeighted Blending.**     *   Instead of training a meta-model (like Ridge), we\nwill determine the optimal weights to combine the predictions from the Stage 1\nmodels.     *   This is achieved by finding the weights that minimize the RMSE\nof the blended OOF predictions against the true target values. This simple,\npowerful technique is less prone to overfitting than a learning-based meta-\nmodel.     *   The final submission will be the result of applying these optimal\nweights to the test set predictions from each model.  ### 1.2. Workflow  1.\n**Data Preprocessing & Advanced Feature Engineering:** Load, clean, and engineer\nan extensive set of features. This will include interaction, polynomial, and\ngroup-based aggregation features. 2.  **Cross-Validation (CV) Setup:** Use a\nconsistent `KFold` (K=10) strategy across all stages for robust validation. 3.\n**Stage 1: Base Model Training Loop (for each model type):**     *\n**Hyperparameter Optimization:** Use `Optuna` to find the best hyperparameters\nfor the model by optimizing the mean CV score over the 10 folds.     *\n**OOF/Test Prediction Generation:** Using the best hyperparameters found, run\nthe 10-fold CV training process again. In this run, store the OOF predictions\nand generate test predictions (averaged across the 10 folds). 4.  **Stage 2:\nBlending and Submission:**     *   Load all the saved OOF and test predictions\nfrom Stage 1.     *   Use an optimization algorithm (`scipy.optimize.minimize`)\non the OOF predictions to find the optimal blending weights.     *   Apply these\nweights to the test predictions to create the final submission file.  ---  ## 2.\nData Processing  We will build upon the solid foundation of the previous plan\n(log transform target, handle outliers, basic imputation) and introduce more\nadvanced feature engineering to uncover deeper patterns.  ### 2.1. Initial Setup\nand Cleaning  1.  **Load & Combine:** Load `train.csv`, `test.csv`. Store\n`test_ID`. Concatenate them for consistent processing. 2.  **Target Transform:**\nApply `numpy.log1p` to `SalePrice`. All modeling will be done on this\ntransformed target. 3.  **Outlier Removal:** As before, remove the two training\npoints with `GrLivArea > 4000` as they are known influential outliers. 4.\n**Standard Imputation:** Implement the same imputation strategy as the previous\nplan:     *   Categorical `NaN` -> `\"None\"` (for `PoolQC`, `Alley`, `Fence`,\netc.).     *   Numerical `NaN` -> `0` (for `GarageArea`, `BsmtFinSF1`, etc.).\n*   Mode imputation for `MSZoning`, `Electrical`, etc.     *   Neighborhood-\nbased median imputation for `LotFrontage`.  ### 2.2. Advanced Feature\nEngineering  This is a key area for improvement.  1.  **Polynomial and\nInteraction Features:**     *   Identify the top ~10 most important features\nfrom a preliminary LightGBM model (e.g., `OverallQual`, `GrLivArea`, `TotalSF`,\n`GarageCars`, `ExterQual`).     *   Create interaction and second-degree\npolynomial features for these top features using\n`sklearn.preprocessing.PolynomialFeatures`. For example, `OverallQual` *\n`TotalSF` can be a very powerful feature.  2.  **Group-based Aggregation\nFeatures:**     *   Create features that capture neighborhood-level statistics.\nFor example, for each house, create:         *   `Neighborhood_MedianQual`: The\nmedian `OverallQual` of houses in its `Neighborhood`.         *\n`Neighborhood_MeanArea`: The mean `GrLivArea` of houses in its `Neighborhood`.\n*   `Neighborhood_Count`: The number of houses in the `Neighborhood`.     *\n**Leakage Prevention:** These features must be calculated *within each CV\ntraining fold*. The statistics should be computed on the fold's training data\nonly and then mapped to both the fold's validation data and the test set.  3.\n**Age and Remodel Features:**     *   `HouseAge`: `YrSold` - `YearBuilt`.     *\n`RemodAge`: `YrSold` - `YearRemodAdd`.     *   `IsNew`: `1` if `YrSold` ==\n`YearBuilt`, else `0`.  4.  **Feature Simplification and Combination:**     *\n`TotalSF`: `TotalBsmtSF` + `1stFlrSF` + `2ndFlrSF`.     *   `TotalBath`: Combine\nall full and half baths.     *   `TotalPorchSF`: Sum of all porch-related areas.\n### 2.3. Feature Transformation and Encoding  1.  **Type Conversion:** Convert\n`MSSubClass`, `OverallCond`, `YrSold`, `MoSold` to string type to be treated as\ncategorical. 2.  **Ordinal Encoding:** Manually map ordinal features with\nexplicit quality scales to numbers (e.g., `ExterQual`, `BsmtQual`). 3.\n**Skewness Correction:** Use `scipy.special.boxcox1p` on all numerical features\nwith skewness > 0.5. 4.  **Categorical Encoding:** Use `pandas.get_dummies` to\none-hot encode all remaining categorical features. 5.  **Scaling:** For\ndistance-based and neural network models (`KNeighborsRegressor`,\n`MLPRegressor`), the data must be scaled. Use\n`sklearn.preprocessing.RobustScaler`, which is less sensitive to outliers. The\nscaler must be `fit` on the training data of each CV fold and then used to\n`transform` the validation and test data.  ---  ## 3. Model Selection  We will\nexpand the model set to introduce more diversity beyond just gradient boosting\ntrees.  ### 3.1. Stage 1: Base Models  *   **LightGBM\n(`lightgbm.LGBMRegressor`):** Still the king of speed and performance on tabular\ndata. *   **XGBoost (`xgboost.XGBRegressor`):** A must-have for its robustness\nand slightly different tree-building approach. *   **CatBoost\n(`catboost.CatBoostRegressor`):** Excellent for its unique handling of\ncategorical features and symmetric tree growth. *   **ElasticNetCV\n(`sklearn.linear_model.ElasticNetCV`):** A regularized linear model that is more\nflexible than Ridge by combining L1 and L2 penalties. It can perform feature\nselection and captures linear trends. *   **HistGradientBoostingRegressor\n(`sklearn.ensemble.HistGradientBoostingRegressor`):** Scikit-learn's\nimplementation inspired by LightGBM. While similar, implementation differences\nprovide valuable diversity. *   **KNeighborsRegressor\n(`sklearn.neighbors.KNeighborsRegressor`):** A non-parametric, instance-based\nmodel. It will capture local relationships missed by other models. Requires\nscaled data.  ---  ## 4. Training Strategy  This section details the core\npipeline for training our models and generating predictions for the blend.  ###\n4.1. Validation Framework  *   **Strategy:** `sklearn.model_selection.KFold` *\n**Configuration:** `n_splits=10`, `shuffle=True`, `random_state=42`.  ### 4.2.\nStage 1: Hyperparameter Optimization and Training  For each of the six models\nlisted above, perform the following procedure:  1.  **Define the Optuna\nObjective Function:**     *   Create a Python function that takes an\n`optuna.Trial` object as input.     *   Inside this function, define the\nhyperparameter search space using `trial.suggest_...()` methods (e.g.,\n`trial.suggest_float('learning_rate', 0.01, 0.1)`).     *   Instantiate the\nmodel with these suggested parameters.     *   Use\n`sklearn.model_selection.cross_val_score` with the 10-fold CV object to evaluate\nthe model. The scoring should be `'neg_root_mean_squared_error'`.     *   The\nfunction should return the mean of the scores from the 10 folds.  2.  **Run the\nOptimization Study:**     *   Create an\n`optuna.create_study(direction='maximize')`.     *   Run\n`study.optimize(objective_function, n_trials=50)`.     *   The best\nhyperparameters will be available in `study.best_params`.  3.  **Generate OOF\nand Test Predictions:**     *   Instantiate the model using the `best_params`\nfound by Optuna.     *   Initialize an OOF prediction array (`oof_preds`) of\nzeros with the same length as the training data, and a test prediction array\n(`test_preds`) of zeros with the same shape as the test set.     *   Loop\nthrough the 10 CV folds:         *   Split data into `X_train_fold`,\n`y_train_fold`, `X_val_fold`, `y_val_fold`.         *   **Crucially, perform\nfold-specific feature engineering here (group-by aggregations, scaling).**\n*   Train the model on `(X_train_fold, y_train_fold)`.         *   Generate\npredictions for `X_val_fold` and store them in the corresponding slice of\n`oof_preds`.         *   Generate predictions for the (transformed) test set and\nadd them to `test_preds` (we will average later).     *   Average the test\npredictions: `test_preds = test_preds / n_splits`.     *   Save the `oof_preds`\nand `test_preds` arrays to disk (e.g., `lgbm_oof.npy`, `lgbm_test.npy`).  ###\n4.3. Stage 2: Optimized Weighted Blending  1.  **Load Predictions:** Load all\nsaved `oof_preds` and `test_preds` from Stage 1. Concatenate the OOF predictions\ninto a matrix `OOF_MATRIX` where each column is a model's predictions.  2.\n**Define Blending Objective Function:**     *   Create a function that takes a\nlist/array of `weights` as input.     *   Calculate the weighted average of the\n`OOF_MATRIX` columns: `blended_oof = np.dot(OOF_MATRIX, weights)`.     *\nCalculate and return the RMSE between `blended_oof` and the true `y_log` target.\n3.  **Find Optimal Weights:**     *   Use `scipy.optimize.minimize` to find the\nweights that minimize the objective function.     *   Provide an initial guess\nfor the weights (e.g., equal weights for all models).     *   Use constraints to\nensure the weights are non-negative (`bounds`) and sum to 1 (`constraints`).  4.\n**Generate Final Predictions:**     *   Concatenate the test predictions into a\n`TEST_MATRIX`.     *   Apply the `optimal_weights` found in the previous step:\n`final_predictions_log = np.dot(TEST_MATRIX, optimal_weights)`.  ---  ## 5.\nEvaluation  ### 5.1. Local Validation  *   **Primary Metric:** The final RMSE\nscore calculated from the optimized blend of OOF predictions (`blended_oof`)\nagainst the true log-transformed target `y_log`. This score is our best estimate\nof leaderboard performance. *   **Secondary Metrics:** We will also report the\nindividual RMSE of each base model's OOF predictions to understand their\nrelative contributions and performance.  ### 5.2. Submission File Generation  1.\nTake the `final_predictions_log` vector. 2.  Inverse-transform the predictions\nusing `numpy.expm1`. 3.  Create the submission DataFrame with `Id` and the final\n`SalePrice` predictions. 4.  Save to `submission.csv` with `index=False`.  ---\n## 6. Potential Issues and Mitigations  *   **Data Leakage in Feature\nEngineering:** The biggest risk is leaking information during the creation of\ngroup-by aggregation features. **Mitigation:** Strictly enforce that these\nfeatures are computed *only* on the training portion of each CV fold and then\napplied to the validation/test sets. Do not compute them on the combined dataset\nbeforehand. *   **Computational Cost:** The hyperparameter optimization step for\neach model is computationally expensive. **Mitigation:** Start with a smaller\n`n_trials` (e.g., 20) for Optuna. If necessary, parallelize the training of\ndifferent base models as they are independent of each other. *   **Overfitting\nthe Blend Weights:** Optimizing weights on the OOF set could theoretically\noverfit. **Mitigation:** Using a high number of folds (K=10) and simple\nconstraints (non-negative, sum to 1) makes this highly unlikely. This method is\ngenerally more robust than training a complex meta-model. *   **Collinearity in\nPredictions:** The base model predictions will be highly correlated.\n**Mitigation:** Our blending approach handles this well. The optimizer might\nassign very low or zero weights to redundant models. An alternative like\n`ElasticNetCV` as a meta-model could also work, but the direct optimization\napproach is often simpler and just as effective.", "Here is a detailed implementation plan for a winning solution to the House\nPrices prediction competition.  ## 1. Approach: Overall Strategy and Model\nArchitecture  The previous approach successfully implemented a standard stacking\nensemble, achieving a strong baseline. To surpass this, we will pivot from\nstacking to an **Optimized Weighted Averaging Ensemble**. This strategy is often\nmore robust, less prone to overfitting a meta-model, and allows for fine-grained\ncontrol over the contribution of each model.  Our new architecture will focus on\nthree core principles: 1.  **Increased Model Diversity:** We will introduce a\nneural network and different model implementations to capture a wider range of\npatterns in the data. 2.  **Advanced Feature Engineering:** We will move beyond\nsimple feature combinations to create more powerful predictive signals through\ninteraction and aggregation features. 3.  **Robust Blending:** Instead of\ntraining a meta-model, we will generate out-of-fold (OOF) predictions and use an\noptimization algorithm to find the optimal weights for a linear blend of our\nmodels, minimizing the overall cross-validated RMSE.  ### 1.1. High-Level\nArchitecture  1.  **Feature Engineering Layer:** A sophisticated pipeline to\ngenerate a rich feature set, including interaction, polynomial, and group-based\naggregation features. 2.  **Base Models Layer:** A diverse set of five models\nwill be trained independently using a consistent cross-validation scheme. The\nset will include gradient boosted trees, a regularized linear model, and a\nneural network. 3.  **Weight Optimization Layer:** OOF predictions from the base\nmodels will be used to find optimal blending weights that minimize the\nensemble's RMSE. 4.  **Inference Layer:** The final test set predictions will be\na weighted average of the individual model predictions on the test set, using\nthe optimized weights.  ### 1.2. Workflow  1.  **Data Preprocessing & Advanced\nFeature Engineering:** Load, clean, and engineer a comprehensive set of\nfeatures. This will be an extension of the previous plan. 2.  **Cross-Validation\nSetup:** A consistent 10-Fold cross-validation strategy will be used for all\nbase models to generate OOF and test predictions. 3.  **Base Model Training:**\n*   Train each of the five base models (LightGBM, XGBoost, ElasticNet,\nHistGradientBoosting, MLP) within the CV loop.     *   For each model, store its\nOOF predictions and the average of its test set predictions across all folds. 4.\n**Optimal Weight Search:**     *   Use the collected OOF predictions as input to\na `scipy.optimize` function.     *   The objective function will calculate the\nRMSE of the weighted average of OOF predictions against the true target.     *\nThe optimizer will find the weights `w_1, w_2, ..., w_5` that minimize this\nobjective function. 5.  **Final Prediction & Submission:**     *   Apply the\noptimized weights to the averaged test set predictions from the base models to\ncompute the final submission file.  This approach directly optimizes for the\ncompetition metric (RMSE) during the ensembling stage, providing a more direct\npath to a high-ranking solution than stacking.  ---  ## 2. Data Processing  We\nwill build upon the solid foundation of the previous plan (log-transforming the\ntarget, handling outliers, and baseline imputation) and introduce more advanced\nfeature engineering techniques.  ### 2.1. Initial Setup  1.  **Load & Combine:**\nLoad `train.csv` and `test.csv`. Store `test_ID`. Concatenate them for\nconsistent processing. 2.  **Target Transform & Outliers:** Apply `numpy.log1p`\nto `SalePrice`. Remove the two training data points with `GrLivArea > 4000`.\n### 2.2. Missing Value Imputation  Follow the same imputation strategy as the\nprevious plan, as it is robust and well-reasoned for this dataset (e.g., \"None\"\nfor absent features, 0 for numerical absences, mode for others, and median-by-\nneighborhood for `LotFrontage`).  ### 2.3. Advanced Feature Engineering  This is\na key area for improvement.  1.  **Interaction and Polynomial Features:**     *\nIdentify the top 10 most important numerical features from a preliminary\nLightGBM run (e.g., `GrLivArea`, `TotalSF`, `OverallQual`, `GarageCars`,\n`YearBuilt`).     *   Create polynomial features (degree 2) for these key\nfeatures using `sklearn.preprocessing.PolynomialFeatures`. This captures non-\nlinear effects.     *   Create explicit interaction features, for example:\n*   `OverallQual_x_TotalSF` = `OverallQual` * `TotalSF`         *\n`YearBuilt_x_GarageCars` = `YearBuilt` * `GarageCars`  2.  **Neighborhood-based\nAggregation Features:** The neighborhood is a strong predictor. We can encode\nmore information about it.     *   For key numerical features (`GrLivArea`,\n`OverallQual`, etc.), calculate the median, mean, and standard deviation for\neach `Neighborhood`.     *   Map these aggregated values back to each house in\nthe dataset.     *   Example Features: `Neighborhood_Median_GrLivArea`,\n`Neighborhood_Mean_OverallQual`.     *   **Leakage Prevention:** These\naggregations must be calculated *only* from the training set and then mapped to\nboth the training and test sets to prevent data leakage.  3.  **Label Encoding\nOrdinal Features:** Convert categorical features with a clear order into\nnumerical values as before (e.g., quality ratings `Ex` -> 5, `Gd` -> 4, etc.).\n4.  **Feature Simplification and Transformation:**     *   Convert `MSSubClass`,\n`OverallCond`, `YrSold`, `MoSold` to string type before encoding.     *\nIdentify numerical features with high skewness (> 0.5) and apply a Box-Cox\ntransformation (`scipy.special.boxcox1p`).  5.  **Final Encoding:**     *   Use\n`pandas.get_dummies` to one-hot encode all remaining categorical features.  ###\n2.4. Feature Scaling (for specific models)  *   For distance-based and neural\nnetwork models (ElasticNet, MLP), the data must be scaled. We will use\n`sklearn.preprocessing.RobustScaler`, which is less sensitive to outliers than\n`StandardScaler`. *   Tree-based models (LGBM, XGBoost, HistGradientBoosting) do\nnot require feature scaling. We will maintain two versions of the dataset:\nscaled and unscaled.  ---  ## 3. Model Selection  The key to a successful\nensemble is diversity. We will select models that learn fundamentally different\ntypes of patterns.  ### 3.1. Base Models  *   **LightGBM\n(`lightgbm.LGBMRegressor`):** The fastest and often most accurate GBDT.\nEssential for its performance. *   **XGBoost (`xgboost.XGBRegressor`):** A\nclassic, robust GBDT. Its level-wise tree growth provides diversity from\nLightGBM's leaf-wise growth. *   **ElasticNet\n(`sklearn.linear_model.ElasticNetCV`):**     *   **Why:** A sophisticated linear\nmodel that combines L1 and L2 regularization. It is more flexible than Ridge and\ncan perform implicit feature selection (by driving some coefficients to zero),\nmaking it a powerful and diverse addition. `ElasticNetCV` will find the best\n`alpha` and `l1_ratio` automatically. Use the scaled dataset. *\n**HistGradientBoostingRegressor\n(`sklearn.ensemble.HistGradientBoostingRegressor`):**     *   **Why:** Scikit-\nlearn's implementation of a LightGBM-style model. While similar in concept, its\nimplementation details, default parameters, and handling of categorical features\n(if enabled) are different enough to provide valuable diversity. *   **MLP\nRegressor (`sklearn.neural_network.MLPRegressor`):**     *   **Why:** This is\nour core diversifying model. A simple multi-layer perceptron (neural network)\ncan capture complex non-linear relationships that tree models might miss. Its\ninclusion forces the ensemble to learn from a completely different\nrepresentation of the data. Use the scaled dataset.  ---  ## 4. Training\nStrategy  ### 4.1. Validation Framework  *   **Strategy:**\n`sklearn.model_selection.KFold` *   **Configuration:** `n_splits=10`,\n`shuffle=True`, `random_state=42`. This ensures robust validation and generation\nof OOF predictions.  ### 4.2. Base Model Training  1.  Create two data matrices:\n`X_unscaled` and `X_scaled` (using `RobustScaler` fitted on training data only).\n2.  Initialize OOF and test prediction arrays for each of the five models. 3.\nLoop through each of the 10 folds:     *   Split the data into training and\nvalidation sets for the fold.     *   For each of the five base models:\n*   Select the appropriate dataset (`X_scaled` for ElasticNet/MLP, `X_unscaled`\nfor trees).         *   Instantiate and train the model on the fold's training\ndata.         *   Use the validation set for early stopping where applicable\n(LGBM, XGBoost, MLP).         *   Predict on the fold's validation data and\nstore in the OOF array.         *   Predict on the full test set and store for\nlater averaging. 4.  After the loop, average the test predictions across all\nfolds for each model to get five final test prediction vectors.  ### 4.3.\nRecommended Hyperparameters (Starting Point)  *   **LGBM/XGBoost:** Use the\nrobust hyperparameters from the previous plan. *   **ElasticNetCV:**     *\n`l1_ratio`: `[.1, .5, .7, .9, .95, .99, 1]`     *   `n_alphas`: `100`     *\n`cv`: `5` (inner CV loop to find best params)     *   `max_iter`: `5000` *\n**HistGradientBoostingRegressor:**     *   `learning_rate`: `0.05`     *\n`max_iter`: `500`     *   `max_leaf_nodes`: `31`     *   `validation_fraction`:\n`0.1` (for early stopping)     *   `n_iter_no_change`: `20` *\n**MLPRegressor:**     *   `hidden_layer_sizes`: `(64, 32)`     *   `activation`:\n`'relu'`     *   `solver`: `'adam'`     *   `alpha`: `0.001` (L2 regularization)\n*   `batch_size`: `32`     *   `learning_rate_init`: `0.001`     *\n`early_stopping`: `True`     *   `n_iter_no_change`: `15`  ### 4.4. Weight\nOptimization  1.  **Objective Function:** Create a Python function that takes a\nlist of weights as input. Inside the function:     *   Calculate the weighted\naverage of the OOF predictions: `ensemble_oof = w1*oof1 + w2*oof2 + ...`.     *\nReturn the RMSE between `ensemble_oof` and the true `y_log` target. 2.\n**Optimization:**     *   Use `scipy.optimize.minimize` to find the weights that\nminimize the objective function.     *   **Initial Guess:** `[0.2, 0.2, 0.2,\n0.2, 0.2]` (equal weights).     *   **Constraints:** Use constraints to ensure\nthe weights are non-negative (`bounds`) and sum to 1 (`constraints`). This\ncreates a stable and interpretable blend.  ---  ## 5. Evaluation  ### 5.1. Local\nValidation  *   **Primary Metric:** The final, optimized RMSE calculated on the\nblended OOF predictions. *   **Procedure:**     1.  Obtain the optimal weights\nfrom the optimization step.     2.  Calculate the final blended OOF prediction:\n`final_oof = opt_w1*oof1 + opt_w2*oof2 + ...`.     3.  Calculate the RMSE\nbetween `final_oof` and `y_log`. This single score is our best estimate of\nleaderboard performance.     4.  For diagnostics, also report the individual OOF\nRMSE for each of the five base models to ensure they are all contributing\nmeaningfully.  ### 5.2. Submission File Generation  1.  Retrieve the five\naveraged test prediction vectors. 2.  Calculate the final test prediction using\nthe optimized weights: `final_test_pred = opt_w1*test1 + opt_w2*test2 + ...`. 3.\nInverse-transform the predictions using `numpy.expm1`. 4.  Create the\n`submission.csv` file with `Id` and the final `SalePrice` predictions.  ---  ##\n6. Potential Issues and Mitigations  *   **Curse of Dimensionality:** Our\nadvanced feature engineering will create many new features. This could introduce\nnoise.     *   **Mitigation:** Before training the final models, run a baseline\nLightGBM and use its feature importance scores to select the top 200-300\nfeatures. Alternatively, use `sklearn.feature_selection.SelectFromModel` with a\n`LassoCV` model to perform automated feature selection. *   **MLP Instability:**\nNeural networks can be sensitive to random initialization.     *\n**Mitigation:** Set a fixed `random_state` in the `MLPRegressor` for\nreproducibility. For a more robust solution, train the MLP multiple times (3-5)\nwith different seeds and average its OOF/test predictions before feeding them to\nthe optimization layer. *   **Weight Optimization Overfitting:** The\noptimization process could potentially overfit to the specific OOF set.     *\n**Mitigation:** This risk is low because the OOF predictions are already cross-\nvalidated. The use of constraints (non-negativity, sum-to-one) also acts as a\npowerful regularizer, preventing any single model from dominating with an\nextreme weight. *   **Data Leakage in Aggregations:** Calculating aggregation\nfeatures (e.g., median price by neighborhood) across the entire combined dataset\nwould be a major leak.     *   **Mitigation:** As stated in the plan, all fit-\nlike operations (scalers, imputers, and especially aggregations) must be\ncomputed *only* on the training portion of the data within each CV fold and then\napplied to the validation/test sets. For simplicity in the initial\nimplementation, aggregations can be fit on the entire training set (`train.csv`)\nand applied to the test set (`test.csv`).", "Here is a detailed implementation plan for a winning solution to the House\nPrices prediction competition.  ## 1. Approach: Overall Strategy and Model\nArchitecture  The previous solution employed a stacking ensemble, which is a\npowerful technique. To create a distinct and potentially more robust solution,\nwe will pivot to an **optimized weighted average ensemble**. This approach is\noften simpler to implement, less prone to overfitting the meta-learner, and can\nbe more stable, especially on smaller datasets like this one.  Our strategy is\nto build a diverse set of high-performing base models, tune them meticulously,\nand then find the optimal linear combination of their predictions based on\ncross-validation performance.  ### 1.1. High-Level Architecture  1.\n**Preprocessing & Feature Engineering:** A single, advanced preprocessing\npipeline will be developed to generate a rich feature set. This includes\ncreating interaction and aggregation-based features. 2.  **Base Model Layer:** A\nportfolio of four diverse models will be trained independently on the full\nfeature set:     *   Two Gradient Boosting Machines (LightGBM, XGBoost)     *\nOne Regularized Linear Model (ElasticNet)     *   One Multi-Layer Perceptron\n(Neural Network) 3.  **Hyperparameter Optimization:** We will use Optuna to\nsystematically find the best hyperparameters for each of the four base models\nbefore final training. 4.  **Ensemble Layer:** The final prediction will be a\nweighted average of the predictions from the base models. The weights will be\ndetermined by optimizing the ensemble's RMSE on the out-of-fold (OOF)\npredictions generated during cross-validation.  ### 1.2. Workflow  1.  **Data\nPreprocessing:** Load, clean, and engineer features. This step is critical and\nwill involve creating new, powerful predictors. 2.  **Hyperparameter Tuning:**\nFor each base model, run an Optuna study to identify the optimal hyperparameter\nconfiguration using cross-validation. 3.  **Cross-Validation Training:**     *\nSet up a `RepeatedKFold` cross-validation scheme for robust evaluation.     *\nWithin each fold, train the four base models using their optimized\nhyperparameters.     *   Generate OOF predictions for the validation set and\npredictions for the test set. 4.  **Weight Optimization:**     *   Use the\ncomplete set of OOF predictions from all models.     *   Employ an optimization\nalgorithm (e.g., `scipy.optimize.minimize`) to find the set of weights that\nminimizes the overall ensemble RMSE on the OOF predictions. 5.  **Final\nPrediction & Submission:**     *   Average the test set predictions from each\nmodel across all folds.     *   Apply the optimized weights to the averaged test\npredictions to get the final submission file.  This approach focuses on model\ndiversity, rigorous tuning, and a stable ensembling method, which are key\ningredients for a winning solution.  ---  ## 2. Data Processing  We will build\nupon the solid foundation of the previous plan but introduce more sophisticated\nfeature engineering to capture complex relationships in the data.  ### 2.1.\nInitial Setup and Cleaning  1.  **Load & Combine:** Load `train.csv` and\n`test.csv` into pandas. Concatenate them for consistent processing, storing\n`test_ID` for submission. 2.  **Target Transformation:** Apply `numpy.log1p` to\nthe `SalePrice` target variable to normalize its distribution. All modeling will\nbe done on this transformed target. 3.  **Outlier Removal:** As before, remove\nthe two training data points with `GrLivArea > 4000` and a low `SalePrice`.  ###\n2.2. Missing Value Imputation  This follows the standard, robust procedure based\non `data_description.txt`: *   **Categorical `NaN` -> `\"None\"`**: For features\nwhere `NaN` means absence (e.g., `PoolQC`, `Alley`, `Fence`, `FireplaceQu`,\n`GarageType`). *   **Numerical `NaN` -> `0`**: For features where `NaN` means\nzero (e.g., `GarageArea`, `TotalBsmtSF`). *   **Mode Imputation**: For features\nwith a few missing values without a specific meaning (e.g., `MSZoning`,\n`Electrical`, `KitchenQual`). *   **Grouped Median Imputation**: For\n`LotFrontage`, impute with the median `LotFrontage` of the corresponding\n`Neighborhood`.  ### 2.3. Advanced Feature Engineering  This is a key area for\nimprovement.  1.  **Polynomial and Interaction Features:**     *   Create\npolynomial features (degree 2) for the most important numerical columns\n(identified via feature importance from a baseline model), such as\n`OverallQual`, `GrLivArea`, `TotalSF`, `GarageCars`, `YearBuilt`. Use\n`sklearn.preprocessing.PolynomialFeatures`.     *   Create explicit interaction\nfeatures that make intuitive sense:         *   `OverallQual_x_TotalSF` =\n`OverallQual` * `TotalSF`         *   `GrLivArea_x_OverallQual` = `GrLivArea` *\n`OverallQual` 2.  **Neighborhood-based Aggregations:**     *   **Crucial Note:**\nThese must be created *within the cross-validation loop* to prevent data\nleakage. For each fold, calculate aggregates on the fold's training data and\nthen map them to both the validation and test sets.     *   Example features:\n*   `Neighborhood_MedGrLivArea`: Median `GrLivArea` per neighborhood.         *\n`Neighborhood_Price_Ratio`: Median `OverallQual` / Median `SalePrice` per\nneighborhood. 3.  **Simplified Features:**     *   Combine existing features as\nin the previous plan: `TotalSF`, `TotalBath`, `TotalPorchSF`.     *   Create\nbinary indicators: `HasPool`, `HasGarage`, `HasBsmt`, `IsNewHouse` (`YearBuilt`\n== `YrSold`). 4.  **Type Conversion:**     *   Convert `MSSubClass`,\n`OverallCond`, `YrSold`, `MoSold` to string/category type before encoding.  ###\n2.4. Feature Transformation and Encoding  1.  **Skew Correction:** Identify\nnumerical features with skew > 0.5 and apply a Box-Cox transformation\n(`scipy.special.boxcox1p`). 2.  **Ordinal Mapping:** Manually map ordinal\nfeatures (e.g., `ExterQual`, `KitchenQual`) to numerical scales (`{'Ex': 5,\n'Gd': 4, ...}`). 3.  **One-Hot Encoding:** Apply one-hot encoding\n(`pandas.get_dummies`) to all remaining categorical features. 4.  **Scaling:**\n*   After all other preprocessing, scale all numerical features using\n`sklearn.preprocessing.RobustScaler`. This is less sensitive to outliers than\n`StandardScaler` and is essential for the ElasticNet and Neural Network models.\n---  ## 3. Model Selection  The strength of an ensemble lies in the diversity of\nits components. We will select models that learn different types of patterns.  *\n**LightGBM (`lightgbm.LGBMRegressor`):**     *   **Why:** State-of-the-art\nperformance for tabular data. Fast and efficient. *   **XGBoost\n(`xgboost.XGBRegressor`):**     *   **Why:** A different flavor of gradient\nboosting. Its level-wise tree growth and regularization provide a robust\nalternative to LightGBM's leaf-wise growth. *   **ElasticNet\n(`sklearn.linear_model.ElasticNetCV`):**     *   **Why:** A powerful linear\nmodel that combines L1 and L2 regularization. It can handle feature collinearity\nand perform implicit feature selection, providing a strong and diverse linear\nbaseline that differs from Ridge. `ElasticNetCV` will find the best `alpha` and\n`l1_ratio` automatically. *   **Multi-Layer Perceptron (MLP) with PyTorch:**\n*   **Why:** Neural networks can capture complex non-linear relationships that\ntree-based models might miss. Its inclusion adds significant diversity. The\narchitecture will be kept simple to avoid overfitting.     *   **Proposed\nArchitecture:**         *   Input Layer -> `BatchNorm1d`         *\nLinear(in_features, 128) -> `ReLU` -> `Dropout(0.3)`         *   Linear(128, 64)\n-> `ReLU` -> `Dropout(0.3)`         *   Linear(64, 1) (Output layer)     *\n**Optimizer:** `AdamW` with weight decay for regularization.     *   **Loss\nFunction:** `MSELoss`.  ---  ## 4. Training Strategy  A rigorous training and\nvalidation process is essential for building a reliable model.  ### 4.1.\nValidation Framework  *   **Strategy:** Repeated K-Fold Cross-Validation. *\n**Library:** `sklearn.model_selection.RepeatedKFold`. *   **Configuration:**\n`n_splits=10`, `n_repeats=3`, `random_state=42`. This provides a very stable\nestimate of model performance by training and validating 30 times on different\ndata shuffles.  ### 4.2. Hyperparameter Optimization with Optuna  Before the\nmain training loop, find optimal hyperparameters for each model. *\n**Procedure:** For each model (LGBM, XGBoost, MLP), create an Optuna study. The\nobjective function should take a set of hyperparameters, train the model, and\nreturn its average RMSE score from a simple 5-fold cross-validation on the\ntraining data. *   **Example Search Space for LGBM:**     *   `n_estimators`:\n`(1000, 5000)`     *   `learning_rate`: `(1e-3, 5e-2, log=True)`     *\n`num_leaves`: `(20, 100)`     *   `reg_alpha`, `reg_lambda`: `(1e-2, 10.0,\nlog=True)`     *   `colsample_bytree`, `subsample`: `(0.5, 1.0)` *   Run each\nstudy for a fixed number of trials (e.g., 50-100) and store the best parameters.\n### 4.3. Model Training (Main CV Loop)  1.  Initialize OOF and test prediction\narrays. 2.  Loop through each of the 30 folds from `RepeatedKFold`:     *\nPerform the fold-dependent feature engineering (neighborhood aggregates).     *\nScale the data using a `RobustScaler` fitted *only* on the fold's training data.\n*   For each of the four models:         *   Instantiate the model with its pre-\ntuned optimal hyperparameters.         *   Train on the fold's training data.\nUse early stopping where applicable (LGBM, XGBoost, MLP) based on performance on\nthe validation set.         *   Generate predictions on the validation set (for\nOOF) and the test set. 3.  Average the OOF predictions and test predictions\nacross all repeats for each split to get a single, stable OOF vector and test\nprediction vector per model.  ### 4.4. Ensemble Weight Optimization  1.  Stack\nthe OOF prediction vectors from the four models into a new matrix `OOF_matrix`\n(shape: `n_train_samples x 4`). 2.  Define an objective function that takes a\nlist of four weights as input, calculates the weighted average of the\n`OOF_matrix` columns, and returns the RMSE against the true `y_log` target. 3.\nUse `scipy.optimize.minimize` to find the weights that minimize this objective\nfunction.     *   **Method:** `SLSQP`     *   **Constraints:**         *   The\nsum of weights must equal 1.         *   Each weight must be between 0 and 1.\n---  ## 5. Evaluation  ### 5.1. Local Validation  *   **Primary Metric:** The\nfinal, optimized RMSE score calculated on the OOF predictions using the\noptimized weights. This is our most reliable estimate of leaderboard\nperformance. *   **Procedure:**     1.  Calculate the weighted average of the\nOOF predictions: `final_oof_preds = w1*oof1 + w2*oof2 + ...`     2.  Calculate\nRMSE: `mean_squared_error(y_log, final_oof_preds, squared=False)`. *   Report\nthis final CV score to track progress. A score around 0.10 or lower should be\nachievable.  ### 5.2. Submission File Generation  1.  Average the test\npredictions for each model across all 30 folds. 2.  Apply the optimized weights\nto these averaged test predictions: `final_test_preds = w1*test1 + w2*test2 +\n...` 3.  Inverse-transform the predictions: `final_prices =\nnumpy.expm1(final_test_preds)`. 4.  Create the `submission.csv` file with the\n`Id` and `SalePrice` columns.  ---  ## 6. Potential Issues and Mitigations  *\n**Data Leakage from Aggregates:** This is the most critical risk. The\nimplementation must ensure that any statistics (like neighborhood medians or\nscaler parameters) are computed *only* on the training data for each fold and\nthen applied to the validation and test sets. *   **Computational Cost:**\nRepeated CV combined with Optuna is computationally expensive.     *\n**Mitigation:** Start with fewer repeats (`n_repeats=1`) and fewer Optuna trials\n(`n_trials=25`). Once the pipeline is stable, increase these values for the\nfinal run. *   **Neural Network Instability:** The MLP's performance can vary\nwith weight initialization.     *   **Mitigation:** Within each fold, consider\ntraining the MLP with 2-3 different random seeds and averaging its predictions.\nThis adds stability at the cost of training time. *   **Overfitting Ensemble\nWeights:** With only four models, this risk is low. However, if more models were\nadded, it could become an issue.     *   **Mitigation:** Ensure the OOF\npredictions come from a robust CV scheme (`RepeatedKFold`). A simpler\nalternative is to use weights derived from ranking the models by their\nindividual CV scores (e.g., 0.4, 0.3, 0.2, 0.1) instead of using a numerical\noptimizer."], "code": ["import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\n\n# 1. Data Loading and Initial Setup\ntrain = pd.read_csv(\"input/train.csv\")\ntest = pd.read_csv(\"input/test.csv\")\n\ntrain_ID = train[\"Id\"]\ntest_ID = test[\"Id\"]\n\ntrain.drop(\"Id\", axis=1, inplace=True)\ntest.drop(\"Id\", axis=1, inplace=True)\n\ntrain = train[train[\"GrLivArea\"] < 4000]\n\ny = train[\"SalePrice\"]\ntrain.drop(\"SalePrice\", axis=1, inplace=True)\n\nall_data = pd.concat((train, test)).reset_index(drop=True)\n\n# 2. Target Transformation\ny_log = np.log1p(y)\n\n# 3. Missing Value Imputation\n# Categorical NaN meaning \"None\"\nfor col in (\n    \"PoolQC\",\n    \"MiscFeature\",\n    \"Alley\",\n    \"Fence\",\n    \"FireplaceQu\",\n    \"GarageType\",\n    \"GarageFinish\",\n    \"GarageQual\",\n    \"GarageCond\",\n    \"BsmtQual\",\n    \"BsmtCond\",\n    \"BsmtExposure\",\n    \"BsmtFinType1\",\n    \"BsmtFinType2\",\n    \"MasVnrType\",\n):\n    all_data[col] = all_data[col].fillna(\"None\")\n\n# Numerical NaN meaning 0\nfor col in (\n    \"GarageYrBlt\",\n    \"GarageArea\",\n    \"GarageCars\",\n    \"BsmtFinSF1\",\n    \"BsmtFinSF2\",\n    \"BsmtUnfSF\",\n    \"TotalBsmtSF\",\n    \"BsmtFullBath\",\n    \"BsmtHalfBath\",\n    \"MasVnrArea\",\n):\n    all_data[col] = all_data[col].fillna(0)\n\n# Mode Imputation\nfor col in (\n    \"MSZoning\",\n    \"Electrical\",\n    \"KitchenQual\",\n    \"Exterior1st\",\n    \"Exterior2nd\",\n    \"SaleType\",\n):\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n\n# Special Cases\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median())\n)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data = all_data.drop([\"Utilities\"], axis=1)\n\n# 4. Feature Engineering\nall_data[\"TotalSF\"] = (\n    all_data[\"TotalBsmtSF\"] + all_data[\"1stFlrSF\"] + all_data[\"2ndFlrSF\"]\n)\nall_data[\"TotalBath\"] = (\n    all_data[\"FullBath\"]\n    + 0.5 * all_data[\"HalfBath\"]\n    + all_data[\"BsmtFullBath\"]\n    + 0.5 * all_data[\"BsmtHalfBath\"]\n)\nall_data[\"TotalPorchSF\"] = (\n    all_data[\"OpenPorchSF\"]\n    + all_data[\"3SsnPorch\"]\n    + all_data[\"EnclosedPorch\"]\n    + all_data[\"ScreenPorch\"]\n    + all_data[\"WoodDeckSF\"]\n)\n\nall_data[\"HasPool\"] = all_data[\"PoolArea\"].apply(lambda x: 1 if x > 0 else 0)\nall_data[\"Has2ndfloor\"] = all_data[\"2ndFlrSF\"].apply(lambda x: 1 if x > 0 else 0)\nall_data[\"HasGarage\"] = all_data[\"GarageArea\"].apply(lambda x: 1 if x > 0 else 0)\nall_data[\"HasBsmt\"] = all_data[\"TotalBsmtSF\"].apply(lambda x: 1 if x > 0 else 0)\nall_data[\"HasFireplace\"] = all_data[\"Fireplaces\"].apply(lambda x: 1 if x > 0 else 0)\n\nall_data[\"MSSubClass\"] = all_data[\"MSSubClass\"].astype(str)\nall_data[\"OverallCond\"] = all_data[\"OverallCond\"].astype(str)\nall_data[\"YrSold\"] = all_data[\"YrSold\"].astype(str)\nall_data[\"MoSold\"] = all_data[\"MoSold\"].astype(str)\n\n# 5. Feature Transformation\n# Ordinal Feature Mapping\nquality_map = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"None\": 0}\nfor col in (\n    \"ExterQual\",\n    \"ExterCond\",\n    \"BsmtQual\",\n    \"BsmtCond\",\n    \"HeatingQC\",\n    \"KitchenQual\",\n    \"FireplaceQu\",\n    \"GarageQual\",\n    \"GarageCond\",\n    \"PoolQC\",\n):\n    all_data[col] = all_data[col].map(quality_map)\n\nexposure_map = {\"Gd\": 4, \"Av\": 3, \"Mn\": 2, \"No\": 1, \"None\": 0}\nall_data[\"BsmtExposure\"] = all_data[\"BsmtExposure\"].map(exposure_map).fillna(0)\n\nfinish_map = {\"GLQ\": 5, \"ALQ\": 4, \"BLQ\": 3, \"Rec\": 2, \"LwQ\": 1, \"Unf\": 0, \"None\": 0}\nfor col in [\"BsmtFinType1\", \"BsmtFinType2\"]:\n    all_data[col] = all_data[col].map(finish_map).fillna(0)\n\ngarage_finish_map = {\"Fin\": 3, \"RFn\": 2, \"Unf\": 1, \"None\": 0}\nall_data[\"GarageFinish\"] = all_data[\"GarageFinish\"].map(garage_finish_map).fillna(0)\n\n# Skewed Feature Correction\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\nskewed_feats = skewed_feats[skewed_feats > 0.5]\nskewed_feats = skewed_feats.index\n\nfor feat in skewed_feats:\n    all_data[feat] = boxcox1p(all_data[feat], 0.15)\n\n# Categorical Feature Encoding\nall_data = pd.get_dummies(all_data)\n\n# 6. Data Splitting\nX = all_data[: len(y_log)]\nX_test = all_data[len(y_log) :]\n\n# 7. Model Training\n# Validation Framework\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# Level 0: Base Models\nlgb_model = lgb.LGBMRegressor(\n    objective=\"regression_l1\",\n    n_estimators=2000,\n    learning_rate=0.01,\n    num_leaves=31,\n    max_depth=-1,\n    reg_alpha=0.1,\n    reg_lambda=0.1,\n    colsample_bytree=0.7,\n    subsample=0.7,\n    n_jobs=-1,\n    random_state=42,\n)\n\nxgb_model = xgb.XGBRegressor(\n    objective=\"reg:squarederror\",\n    eval_metric=\"rmse\",\n    n_estimators=2000,\n    learning_rate=0.01,\n    max_depth=4,\n    colsample_bytree=0.7,\n    subsample=0.7,\n    reg_alpha=0.005,\n    n_jobs=-1,\n    random_state=42,\n    early_stopping_rounds=100,\n)\n\ncat_model = CatBoostRegressor(\n    iterations=3000,\n    learning_rate=0.02,\n    depth=6,\n    l2_leaf_reg=3,\n    loss_function=\"RMSE\",\n    early_stopping_rounds=100,\n    verbose=False,\n    random_state=42,\n)\n\nridge_model = RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0], cv=None)\n\n# Level 0 Training and Prediction\nlgb_oof = np.zeros(X.shape[0])\nlgb_test_preds = np.zeros(X_test.shape[0])\n\nxgb_oof = np.zeros(X.shape[0])\nxgb_test_preds = np.zeros(X_test.shape[0])\n\ncat_oof = np.zeros(X.shape[0])\ncat_test_preds = np.zeros(X_test.shape[0])\n\nridge_oof = np.zeros(X.shape[0])\nridge_test_preds = np.zeros(X_test.shape[0])\n\nfor fold, (train_index, val_index) in enumerate(kf.split(X, y_log)):\n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_val = y_log.iloc[train_index], y_log.iloc[val_index]\n\n    # LightGBM\n    lgb_model.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_val, y_val)],\n        callbacks=[lgb.early_stopping(100, verbose=False)],\n    )\n    lgb_oof[val_index] = lgb_model.predict(X_val)\n    lgb_test_preds += lgb_model.predict(X_test) / kf.n_splits\n\n    # XGBoost\n    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    xgb_oof[val_index] = xgb_model.predict(X_val)\n    xgb_test_preds += xgb_model.predict(X_test) / kf.n_splits\n\n    # CatBoost\n    cat_model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n    cat_oof[val_index] = cat_model.predict(X_val)\n    cat_test_preds += cat_model.predict(X_test) / kf.n_splits\n\n    # Ridge\n    ridge_model.fit(X_train, y_train)\n    ridge_oof[val_index] = ridge_model.predict(X_val)\n    ridge_test_preds += ridge_model.predict(X_test) / kf.n_splits\n\n# Level 1: Meta-Model\nX_meta = np.column_stack((lgb_oof, xgb_oof, cat_oof, ridge_oof))\nX_test_meta = np.column_stack(\n    (lgb_test_preds, xgb_test_preds, cat_test_preds, ridge_test_preds)\n)\n\nmeta_model = RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0], cv=None)\nmeta_model.fit(X_meta, y_log)\nfinal_predictions = meta_model.predict(X_test_meta)\n\n# 8. Evaluation\n# Local Validation\noof_predictions = meta_model.predict(X_meta)\nrmse = np.sqrt(mean_squared_error(y_log, oof_predictions))\nprint(f\"RMSE on OOF Predictions: {rmse}\")\n\n# 9. Submission File Generation\nfinal_predictions = np.expm1(final_predictions)\nsubmission = pd.DataFrame({\"Id\": test_ID, \"SalePrice\": final_predictions})\nsubmission.to_csv(\"working/submission.csv\", index=False)", "import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.special import boxcox1p\nimport optuna\n\n# 1. Data Ingestion & Initial Cleaning\ntrain = pd.read_csv(\"input/train.csv\")\ntest = pd.read_csv(\"input/test.csv\")\nall_data = pd.concat([train.drop(\"SalePrice\", axis=1), test], axis=0)\ntrain_idx = train.index\ntest_idx = test.index\n\n# Target Transformation\ny_log = np.log1p(train[\"SalePrice\"])\n\n# Outlier Removal\nall_data = all_data.drop(\n    train[(train[\"GrLivArea\"] > 4000) & (train[\"SalePrice\"] < 300000)].index\n)\ny_log = y_log.drop(\n    train[(train[\"GrLivArea\"] > 4000) & (train[\"SalePrice\"] < 300000)].index\n)\ntrain_idx = all_data.iloc[: len(y_log)].index\n\n# Initial Imputation & Feature Creation\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median())\n)\nfor col in (\"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\"):\n    all_data[col] = all_data[col].fillna(\"None\")\nfor col in (\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\"):\n    all_data[col] = all_data[col].fillna(\"None\")\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data[\"Electrical\"] = all_data[\"Electrical\"].fillna(all_data[\"Electrical\"].mode()[0])\nall_data[\"TotalSF\"] = (\n    all_data[\"TotalBsmtSF\"] + all_data[\"1stFlrSF\"] + all_data[\"2ndFlrSF\"]\n)\nall_data[\"TotalBath\"] = (\n    all_data[\"FullBath\"]\n    + (0.5 * all_data[\"HalfBath\"])\n    + all_data[\"BsmtFullBath\"]\n    + (0.5 * all_data[\"BsmtHalfBath\"])\n)\nall_data[\"TotalPorchSF\"] = (\n    all_data[\"OpenPorchSF\"]\n    + all_data[\"EnclosedPorch\"]\n    + all_data[\"3SsnPorch\"]\n    + all_data[\"ScreenPorch\"]\n)\nall_data[\"HasPool\"] = all_data[\"PoolArea\"].apply(lambda x: 1 if x > 0 else 0)\nall_data[\"HasBsmt\"] = all_data[\"TotalBsmtSF\"].apply(lambda x: 1 if x > 0 else 0)\nall_data = all_data.fillna(all_data.median())\n\n# Type Conversion\nall_data[\"MSSubClass\"] = all_data[\"MSSubClass\"].astype(str)\nall_data[\"MoSold\"] = all_data[\"MoSold\"].astype(str)\n\n\n# 2.2 Pipeline A: For Tree-Based Models\ndef pipeline_a(df, train_idx, test_idx, target=None, fit_encoder=True):\n    df = df.copy()\n\n    # Feature Engineering\n    neighborhood_median_price = (\n        df.iloc[train_idx].groupby(\"Neighborhood\")[target].median()\n        if target is not None\n        else df.groupby(\"Neighborhood\")[\"LotArea\"].median()\n    )  # Using LotArea as a proxy if target is None\n    df[\"Neighborhood_Med_Price\"] = df[\"Neighborhood\"].map(neighborhood_median_price)\n    df[\"OverallQual_TotalSF\"] = df[\"OverallQual\"] * df[\"TotalSF\"]\n    df[\"GrLivArea_Neighborhood_Med_Price\"] = (\n        df[\"GrLivArea\"] * df[\"Neighborhood_Med_Price\"]\n    )\n\n    # Target Encoding (inside CV loop)\n    categorical_cols = [col for col in df.columns if df[col].dtype == \"object\"]\n    high_cardinality_cols = [\"Neighborhood\", \"Exterior1st\", \"Exterior2nd\"]\n    other_categorical_cols = [\n        col for col in categorical_cols if col not in high_cardinality_cols\n    ]\n\n    if fit_encoder:\n        global target_encoders\n        target_encoders = {}\n        for col in high_cardinality_cols:\n            if col in df.columns:\n                target_encoders[col] = (\n                    df.iloc[train_idx].groupby(col)[target].mean().to_dict()\n                )\n                df[col] = df[col].map(target_encoders[col])\n    else:\n        for col in high_cardinality_cols:\n            if col in df.columns and col in target_encoders:\n                df[col] = (\n                    df[col].map(target_encoders[col]).fillna(0)\n                )  # Handle unseen values\n\n    # Categorical Encoding\n    df = pd.get_dummies(df, columns=other_categorical_cols, dummy_na=False)\n\n    return df\n\n\n# 2.3 Pipeline B: For Linear & NN Models\ndef pipeline_b(df):\n    df = df.copy()\n\n    # Feature Engineering (same as Pipeline A)\n    neighborhood_median_price = df.groupby(\"Neighborhood\")[\n        \"LotArea\"\n    ].median()  # Using LotArea as a proxy\n    df[\"Neighborhood_Med_Price\"] = df[\"Neighborhood\"].map(neighborhood_median_price)\n    df[\"OverallQual_TotalSF\"] = df[\"OverallQual\"] * df[\"TotalSF\"]\n    df[\"GrLivArea_Neighborhood_Med_Price\"] = (\n        df[\"GrLivArea\"] * df[\"Neighborhood_Med_Price\"]\n    )\n\n    # Categorical Encoding\n    categorical_cols = [col for col in df.columns if df[col].dtype == \"object\"]\n    encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n    encoded_data = encoder.fit_transform(df[categorical_cols])\n    encoded_df = pd.DataFrame(\n        encoded_data,\n        index=df.index,\n        columns=encoder.get_feature_names_out(categorical_cols),\n    )\n    df = pd.concat([df.drop(categorical_cols, axis=1), encoded_df], axis=1)\n\n    # Skew Correction\n    numerical_cols = df.select_dtypes(include=np.number).columns\n    skewed_features = df[numerical_cols].apply(lambda x: x.skew())\n    skewed_features = skewed_features[abs(skewed_features) > 0.5]\n    for feature in skewed_features.index:\n        df[feature] = boxcox1p(df[feature], 0.15)\n\n    # Scaling\n    scaler = RobustScaler()\n    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n\n    return df\n\n\n# 4.1 Hyperparameter Optimization with Optuna\ndef objective_lgbm(trial):\n    lgbm_params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"random_state\": 42,\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 50),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.001, 1.0),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.001, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n    }\n    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n    rmse_scores = []\n    for train_idx, val_idx in kf.split(all_data.iloc[train_idx]):\n        X_train = pipeline_a(\n            all_data.copy(),\n            all_data.iloc[train_idx].iloc[train_idx].index,\n            all_data.iloc[train_idx].iloc[val_idx].index,\n            target=y_log.iloc[train_idx],\n        )\n        X_val = pipeline_a(\n            all_data.copy(),\n            all_data.iloc[train_idx].iloc[train_idx].index,\n            all_data.iloc[train_idx].iloc[val_idx].index,\n            target=y_log.iloc[train_idx],\n            fit_encoder=False,\n        )\n        X_train, X_val = X_train.iloc[train_idx], X_val.iloc[val_idx]\n        y_train, y_val = (\n            y_log.iloc[train_idx].iloc[train_idx],\n            y_log.iloc[train_idx].iloc[val_idx],\n        )\n\n        common_cols = list(set(X_train.columns) & set(X_val.columns))\n        X_train = X_train[common_cols]\n        X_val = X_val[common_cols]\n\n        model = lgb.LGBMRegressor(**lgbm_params)\n        model.fit(X_train, y_train)\n        preds = model.predict(X_val)\n        rmse = np.sqrt(mean_squared_error(y_val, preds))\n        rmse_scores.append(rmse)\n    return np.mean(rmse_scores)\n\n\ndef objective_xgb(trial):\n    xgb_params = {\n        \"objective\": \"reg:squarederror\",\n        \"eval_metric\": \"rmse\",\n        \"random_state\": 42,\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.001, 1.0),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.001, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n    }\n    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n    rmse_scores = []\n    for train_idx, val_idx in kf.split(all_data.iloc[train_idx]):\n        X_train = pipeline_a(\n            all_data.copy(),\n            all_data.iloc[train_idx].iloc[train_idx].index,\n            all_data.iloc[train_idx].iloc[val_idx].index,\n            target=y_log.iloc[train_idx],\n        )\n        X_val = pipeline_a(\n            all_data.copy(),\n            all_data.iloc[train_idx].iloc[train_idx].index,\n            all_data.iloc[train_idx].iloc[val_idx].index,\n            target=y_log.iloc[train_idx],\n            fit_encoder=False,\n        )\n        X_train, X_val = X_train.iloc[train_idx], X_val.iloc[val_idx]\n        y_train, y_val = (\n            y_log.iloc[train_idx].iloc[train_idx],\n            y_log.iloc[train_idx].iloc[val_idx],\n        )\n\n        common_cols = list(set(X_train.columns) & set(X_val.columns))\n        X_train = X_train[common_cols]\n        X_val = X_val[common_cols]\n\n        model = xgb.XGBRegressor(**xgb_params)\n        model.fit(X_train, y_train)\n        preds = model.predict(X_val)\n        rmse = np.sqrt(mean_squared_error(y_val, preds))\n        rmse_scores.append(rmse)\n    return np.mean(rmse_scores)\n\n\ndef objective_catboost(trial):\n    catboost_params = {\n        \"loss_function\": \"RMSE\",\n        \"eval_metric\": \"RMSE\",\n        \"random_state\": 42,\n        \"verbose\": 0,\n        \"iterations\": trial.suggest_int(\"iterations\", 200, 1000),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 3, 7),\n        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.001, 1.0),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.6, 1.0),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n    }\n    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n    rmse_scores = []\n    for train_idx, val_idx in kf.split(all_data.iloc[train_idx]):\n        X_train = pipeline_a(\n            all_data.copy(),\n            all_data.iloc[train_idx].iloc[train_idx].index,\n            all_data.iloc[train_idx].iloc[val_idx].index,\n            target=y_log.iloc[train_idx],\n        )\n        X_val = pipeline_a(\n            all_data.copy(),\n            all_data.iloc[train_idx].iloc[train_idx].index,\n            all_data.iloc[train_idx].iloc[val_idx].index,\n            target=y_log.iloc[train_idx],\n            fit_encoder=False,\n        )\n        X_train, X_val = X_train.iloc[train_idx], X_val.iloc[val_idx]\n        y_train, y_val = (\n            y_log.iloc[train_idx].iloc[train_idx],\n            y_log.iloc[train_idx].iloc[val_idx],\n        )\n\n        common_cols = list(set(X_train.columns) & set(X_val.columns))\n        X_train = X_train[common_cols]\n        X_val = X_val[common_cols]\n\n        model = cb.CatBoostRegressor(**catboost_params)\n        model.fit(X_train, y_train)\n        preds = model.predict(X_val)\n        rmse = np.sqrt(mean_squared_error(y_val, preds))\n        rmse_scores.append(rmse)\n    return np.mean(rmse_scores)\n\n\ndef objective_mlp(trial):\n    mlp_params = {\n        \"hidden_layer_sizes\": trial.suggest_categorical(\n            \"hidden_layer_sizes\", [(64,), (128,), (64, 32), (128, 64)]\n        ),\n        \"activation\": trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"]),\n        \"solver\": trial.suggest_categorical(\"solver\", [\"adam\", \"lbfgs\"]),\n        \"alpha\": trial.suggest_float(\"alpha\", 0.0001, 0.01),\n        \"learning_rate\": trial.suggest_categorical(\n            \"learning_rate\", [\"constant\", \"adaptive\"]\n        ),\n        \"random_state\": 42,\n        \"max_iter\": 200,\n        \"early_stopping\": True,\n    }\n    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n    rmse_scores = []\n    for train_idx, val_idx in kf.split(all_data.iloc[train_idx]):\n        X_train = pipeline_b(all_data.copy()).iloc[train_idx]\n        X_val = pipeline_b(all_data.copy()).iloc[val_idx]\n        y_train, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]\n\n        common_cols = list(set(X_train.columns) & set(X_val.columns))\n        X_train = X_train[common_cols]\n        X_val = X_val[common_cols]\n\n        model = MLPRegressor(**mlp_params)\n        model.fit(X_train, y_train)\n        preds = model.predict(X_val)\n        rmse = np.sqrt(mean_squared_error(y_val, preds))\n        rmse_scores.append(rmse)\n    return np.mean(rmse_scores)\n\n\n# Optuna studies (reduced trials for faster execution)\nstudy_lgbm = optuna.create_study(direction=\"minimize\")\nstudy_lgbm.optimize(objective_lgbm, n_trials=10)\nlgbm_best_params = study_lgbm.best_params\n\nstudy_xgb = optuna.create_study(direction=\"minimize\")\nstudy_xgb.optimize(objective_xgb, n_trials=10)\nxgb_best_params = study_xgb.best_params\n\nstudy_catboost = optuna.create_study(direction=\"minimize\")\nstudy_catboost.optimize(objective_catboost, n_trials=10)\ncatboost_best_params = study_catboost.best_params\n\nstudy_mlp = optuna.create_study(direction=\"minimize\")\nstudy_mlp.optimize(objective_mlp, n_trials=10)\nmlp_best_params = study_mlp.best_params\n\n# 4.2 Main Training Loop (10-Fold CV)\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Initialize arrays for OOF and test predictions\noof_lgbm = np.zeros(len(y_log))\noof_xgb = np.zeros(len(y_log))\noof_catboost = np.zeros(len(y_log))\noof_elasticnet = np.zeros(len(y_log))\noof_mlp = np.zeros(len(y_log))\n\ntest_lgbm = np.zeros(len(test))\ntest_xgb = np.zeros(len(test))\ntest_catboost = np.zeros(len(test))\ntest_elasticnet = np.zeros(len(test))\ntest_mlp = np.zeros(len(test))\n\n# Store target encoders\ntarget_encoders = {}\n\n# Loop through the K-Fold splits\nfor fold, (train_idx_fold, val_idx_fold) in enumerate(\n    kf.split(all_data.iloc[train_idx])\n):\n    print(f\"Fold {fold + 1}\")\n\n    # Pipeline A\n    X_train_A = pipeline_a(\n        all_data.copy(),\n        all_data.iloc[train_idx].iloc[train_idx_fold].index,\n        all_data.iloc[train_idx].iloc[val_idx_fold].index,\n        target=y_log.iloc[train_idx],\n    )\n    X_val_A = pipeline_a(\n        all_data.copy(),\n        all_data.iloc[train_idx].iloc[train_idx_fold].index,\n        all_data.iloc[train_idx].iloc[val_idx_fold].index,\n        target=y_log.iloc[train_idx],\n        fit_encoder=False,\n    )\n    X_test_A = pipeline_a(\n        all_data.copy(),\n        all_data.iloc[train_idx].iloc[train_idx_fold].index,\n        all_data.iloc[train_idx].iloc[val_idx_fold].index,\n        target=y_log.iloc[train_idx],\n        fit_encoder=False,\n    )\n\n    X_train_A, X_val_A = (\n        X_train_A.iloc[train_idx].iloc[train_idx_fold],\n        X_val_A.iloc[train_idx].iloc[val_idx_fold],\n    )\n    y_train_fold, y_val_fold = (\n        y_log.iloc[train_idx].iloc[train_idx_fold],\n        y_log.iloc[train_idx].iloc[val_idx_fold],\n    )\n    X_test_A = X_test_A.iloc[test_idx]\n\n    # Pipeline B\n    X_train_B = pipeline_b(all_data.copy()).iloc[train_idx].iloc[train_idx_fold]\n    X_val_B = pipeline_b(all_data.copy()).iloc[train_idx].iloc[val_idx_fold]\n    X_test_B = pipeline_b(all_data.copy()).iloc[test_idx]\n    y_train_fold, y_val_fold = (\n        y_log.iloc[train_idx].iloc[train_idx_fold],\n        y_log.iloc[train_idx].iloc[val_idx_fold],\n    )\n\n    # Align columns\n    common_cols_A_train_val = list(set(X_train_A.columns) & set(X_val_A.columns))\n    X_train_A = X_train_A[common_cols_A_train_val]\n    X_val_A = X_val_A[common_cols_A_train_val]\n    common_cols_A_train_test = list(set(X_train_A.columns) & set(X_test_A.columns))\n    X_train_A = X_train_A[common_cols_A_train_test]\n    X_test_A = X_test_A[common_cols_A_train_test]\n\n    common_cols_B_train_val = list(set(X_train_B.columns) & set(X_val_B.columns))\n    X_train_B = X_train_B[common_cols_B_train_val]\n    X_val_B = X_val_B[common_cols_B_train_val]\n    common_cols_B_train_test = list(set(X_train_B.columns) & set(X_test_B.columns))\n    X_train_B = X_train_B[common_cols_B_train_test]\n    X_test_B = X_test_B[common_cols_B_train_test]\n\n    # Model Training & Prediction\n    # LightGBM\n    lgbm_params = {\n        **lgbm_best_params,\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"random_state\": 42,\n    }\n    model_lgbm = lgb.LGBMRegressor(**lgbm_params)\n    model_lgbm.fit(X_train_A, y_train_fold)\n    oof_lgbm[all_data.iloc[train_idx].iloc[val_idx_fold].index] = model_lgbm.predict(\n        X_val_A\n    )\n    test_lgbm += model_lgbm.predict(X_test_A) / n_splits\n\n    # XGBoost\n    xgb_params = {\n        **xgb_best_params,\n        \"objective\": \"reg:squarederror\",\n        \"eval_metric\": \"rmse\",\n        \"random_state\": 42,\n    }\n    model_xgb = xgb.XGBRegressor(**xgb_params)\n    model_xgb.fit(X_train_A, y_train_fold)\n    oof_xgb[all_data.iloc[train_idx].iloc[val_idx_fold].index] = model_xgb.predict(\n        X_val_A\n    )\n    test_xgb += model_xgb.predict(X_test_A) / n_splits\n\n    # CatBoost\n    catboost_params = {\n        **catboost_best_params,\n        \"loss_function\": \"RMSE\",\n        \"eval_metric\": \"RMSE\",\n        \"random_state\": 42,\n        \"verbose\": 0,\n    }\n    model_catboost = cb.CatBoostRegressor(**catboost_params)\n    model_catboost.fit(X_train_A, y_train_fold)\n    oof_catboost[all_data.iloc[train_idx].iloc[val_idx_fold].index] = (\n        model_catboost.predict(X_val_A)\n    )\n    test_catboost += model_catboost.predict(X_test_A) / n_splits\n\n    # ElasticNet\n    model_elasticnet = ElasticNetCV(\n        l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 0.995, 1],\n        eps=0.001,\n        n_alphas=100,\n        cv=3,\n        random_state=42,\n    )\n    model_elasticnet.fit(X_train_B, y_train_fold)\n    oof_elasticnet[all_data.iloc[train_idx].iloc[val_idx_fold].index] = (\n        model_elasticnet.predict(X_val_B)\n    )\n    test_elasticnet += model_elasticnet.predict(X_test_B) / n_splits\n\n    # MLP Regressor\n    mlp_params = {\n        **mlp_best_params,\n        \"random_state\": 42,\n        \"max_iter\": 200,\n        \"early_stopping\": True,\n    }\n    model_mlp = MLPRegressor(**mlp_params)\n    model_mlp.fit(X_train_B, y_train_fold)\n    oof_mlp[all_data.iloc[train_idx].iloc[val_idx_fold].index] = model_mlp.predict(\n        X_val_B\n    )\n    test_mlp += model_mlp.predict(X_test_B) / n_splits\n\n# 4.3 Level 1 Model Training\nX_meta_train = np.column_stack(\n    (oof_lgbm, oof_xgb, oof_catboost, oof_elasticnet, oof_mlp)\n)\nX_meta_test = np.column_stack(\n    (test_lgbm, test_xgb, test_catboost, test_elasticnet, test_mlp)\n)\n\n# Train Meta-Model\nlgbm_meta_params = {\n    \"n_estimators\": 500,\n    \"learning_rate\": 0.01,\n    \"num_leaves\": 15,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"random_state\": 42,\n}\nmeta_model = lgb.LGBMRegressor(**lgbm_meta_params)\nmeta_model.fit(X_meta_train, y_log)\n\n# Final Stacked Prediction\nstacked_preds = meta_model.predict(X_meta_test)\n\n# 5. Evaluation & Submission\n# 5.1. Local Validation\noof_predictions = meta_model.predict(X_meta_train)\nrmse_stacked = np.sqrt(mean_squared_error(y_log, oof_predictions))\nprint(f\"Stacked RMSE: {rmse_stacked}\")\n\n# 5.2. Post-processing and Submission\n# Blending\nfinal_preds = 0.7 * stacked_preds + 0.3 * test_lgbm\n\n# Inverse Transformation\nfinal_preds = np.expm1(final_preds)\n\n# File Generation\nsubmission = pd.DataFrame({\"Id\": test[\"Id\"], \"SalePrice\": final_preds})\nsubmission.to_csv(\"working/submission.csv\", index=False)", "import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom scipy.special import boxcox1p\nfrom scipy.stats import skew\nimport optuna\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.optimize import minimize\nimport os\n\n# Load data\ntrain = pd.read_csv(\"input/train.csv\")\ntest = pd.read_csv(\"input/test.csv\")\ntest_ID = test[\"Id\"]\ndata = pd.concat([train.drop([\"SalePrice\"], axis=1), test], axis=0)\n\n# Target transform\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny_log = train[\"SalePrice\"]\n\n# Outlier removal\ntrain = train[train[\"GrLivArea\"] < 4000]\ndata = data.iloc[train.index]\ny_log = y_log.iloc[train.index]\n\n# Imputation\nfor col in [\n    \"PoolQC\",\n    \"Alley\",\n    \"Fence\",\n    \"MiscFeature\",\n    \"FireplaceQu\",\n    \"GarageType\",\n    \"GarageFinish\",\n    \"GarageQual\",\n    \"GarageCond\",\n    \"BsmtQual\",\n    \"BsmtCond\",\n    \"BsmtExposure\",\n    \"BsmtFinType1\",\n    \"BsmtFinType2\",\n    \"MasVnrType\",\n]:\n    data[col] = data[col].fillna(\"None\")\n\nfor col in [\n    \"GarageYrBlt\",\n    \"GarageArea\",\n    \"GarageCars\",\n    \"BsmtFinSF1\",\n    \"BsmtFinSF2\",\n    \"BsmtUnfSF\",\n    \"TotalBsmtSF\",\n    \"BsmtFullBath\",\n    \"BsmtHalfBath\",\n    \"MasVnrArea\",\n]:\n    data[col] = data[col].fillna(0)\n\ndata[\"MSZoning\"] = data[\"MSZoning\"].fillna(data[\"MSZoning\"].mode()[0])\ndata[\"Electrical\"] = data[\"Electrical\"].fillna(data[\"Electrical\"].mode()[0])\ndata[\"KitchenQual\"] = data[\"KitchenQual\"].fillna(data[\"KitchenQual\"].mode()[0])\ndata[\"Functional\"] = data[\"Functional\"].fillna(data[\"Functional\"].mode()[0])\ndata[\"Exterior1st\"] = data[\"Exterior1st\"].fillna(data[\"Exterior1st\"].mode()[0])\ndata[\"Exterior2nd\"] = data[\"Exterior2nd\"].fillna(data[\"Exterior2nd\"].mode()[0])\ndata[\"SaleType\"] = data[\"SaleType\"].fillna(data[\"SaleType\"].mode()[0])\n\ndata[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median())\n)\n\n# Feature Engineering\ndata[\"HouseAge\"] = data[\"YrSold\"] - data[\"YearBuilt\"]\ndata[\"RemodAge\"] = data[\"YrSold\"] - data[\"YearRemodAdd\"]\ndata[\"IsNew\"] = (data[\"YrSold\"] == data[\"YearBuilt\"]).astype(int)\ndata[\"TotalSF\"] = data[\"TotalBsmtSF\"] + data[\"1stFlrSF\"] + data[\"2ndFlrSF\"]\ndata[\"TotalBath\"] = (\n    data[\"FullBath\"]\n    + 0.5 * data[\"HalfBath\"]\n    + data[\"BsmtFullBath\"]\n    + 0.5 * data[\"BsmtHalfBath\"]\n)\ndata[\"TotalPorchSF\"] = (\n    data[\"OpenPorchSF\"]\n    + data[\"EnclosedPorch\"]\n    + data[\"3SsnPorch\"]\n    + data[\"ScreenPorch\"]\n)\n\n# Type conversion\nfor col in [\"MSSubClass\", \"OverallCond\", \"YrSold\", \"MoSold\"]:\n    data[col] = data[col].astype(str)\n\n# Ordinal Encoding\nquality_map = {\"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\nfor col in [\n    \"ExterQual\",\n    \"ExterCond\",\n    \"BsmtQual\",\n    \"BsmtCond\",\n    \"HeatingQC\",\n    \"KitchenQual\",\n    \"FireplaceQu\",\n    \"GarageQual\",\n    \"GarageCond\",\n    \"PoolQC\",\n]:\n    data[col] = data[col].map(quality_map)\n\nexposure_map = {\"None\": 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}\ndata[\"BsmtExposure\"] = data[\"BsmtExposure\"].map(exposure_map)\n\nfinish_map = {\"None\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\nfor col in [\"BsmtFinType1\", \"BsmtFinType2\"]:\n    data[col] = data[col].map(finish_map)\n\n# Skewness correction\nnumeric_cols = data.dtypes[data.dtypes != \"object\"].index\nskewed_cols = data[numeric_cols].apply(lambda x: skew(x.dropna()))\nskewed_cols = skewed_cols[skewed_cols > 0.5]\nfor col in skewed_cols.index:\n    data[col] = boxcox1p(data[col], 0.15)\n\n# Categorical Encoding\ndata = pd.get_dummies(data)\n\n# Polynomial Features\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\ntop_features = [\"OverallQual\", \"GrLivArea\", \"TotalSF\", \"GarageCars\", \"ExterQual\"]\npoly_features = poly.fit_transform(data[top_features])\npoly_df = pd.DataFrame(\n    poly_features,\n    index=data.index,\n    columns=[f\"poly_{i}\" for i in range(poly_features.shape[1])],\n)\ndata = pd.concat([data, poly_df], axis=1)\n\n# Split data\nX = data.iloc[: len(train)]\nX_test = data.iloc[len(train) :]\n\n# Model Training\nn_splits = 10\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n\n# --- Model Definitions ---\ndef create_lgbm(trial):\n    return lgb.LGBMRegressor(\n        objective=\"regression\",\n        random_state=42,\n        n_estimators=trial.suggest_int(\"n_estimators\", 100, 1000),\n        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n        num_leaves=trial.suggest_int(\"num_leaves\", 20, 50),\n        max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n        min_child_samples=trial.suggest_int(\"min_child_samples\", 5, 50),\n        subsample=trial.suggest_float(\"subsample\", 0.7, 1.0),\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.7, 1.0),\n        reg_alpha=trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n        reg_lambda=trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n        n_jobs=-1,\n    )\n\n\ndef create_xgb(trial):\n    return xgb.XGBRegressor(\n        objective=\"reg:squarederror\",\n        random_state=42,\n        n_estimators=trial.suggest_int(\"n_estimators\", 100, 1000),\n        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n        max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n        subsample=trial.suggest_float(\"subsample\", 0.7, 1.0),\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.7, 1.0),\n        reg_alpha=trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n        reg_lambda=trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n        n_jobs=-1,\n        early_stopping_rounds=50,\n    )\n\n\ndef create_catboost(trial):\n    return cb.CatBoostRegressor(\n        random_state=42,\n        verbose=0,\n        iterations=trial.suggest_int(\"iterations\", 100, 1000),\n        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n        depth=trial.suggest_int(\"depth\", 3, 10),\n        l2_leaf_reg=trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n        border_count=trial.suggest_int(\"border_count\", 20, 100),\n    )\n\n\ndef create_elasticnet(trial):\n    return ElasticNetCV(\n        random_state=42,\n        l1_ratio=trial.suggest_float(\"l1_ratio\", 0.1, 0.9),\n        alphas=[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 1],\n    )\n\n\ndef create_histgbm(trial):\n    return HistGradientBoostingRegressor(\n        random_state=42,\n        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n        max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n        l2_regularization=trial.suggest_float(\"l2_regularization\", 1e-8, 1.0),\n        min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 5, 50),\n    )\n\n\ndef create_knn(trial):\n    return KNeighborsRegressor(\n        n_neighbors=trial.suggest_int(\"n_neighbors\", 2, 10),\n        weights=trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n        p=trial.suggest_int(\"p\", 1, 2),\n    )\n\n\n# --- Optuna Objective Functions ---\ndef objective_lgbm(trial):\n    model = create_lgbm(trial)\n    scores = cross_val_score(\n        model, X, y_log, cv=kf, scoring=\"neg_root_mean_squared_error\", n_jobs=-1\n    )\n    return np.mean(scores)\n\n\ndef objective_xgb(trial):\n    model = create_xgb(trial)\n    scores = cross_val_score(\n        model, X, y_log, cv=kf, scoring=\"neg_root_mean_squared_error\", n_jobs=-1\n    )\n    return np.mean(scores)\n\n\ndef objective_catboost(trial):\n    model = create_catboost(trial)\n    scores = cross_val_score(\n        model, X, y_log, cv=kf, scoring=\"neg_root_mean_squared_error\", n_jobs=-1\n    )\n    return np.mean(scores)\n\n\ndef objective_elasticnet(trial):\n    model = create_elasticnet(trial)\n    scores = cross_val_score(\n        model, X, y_log, cv=kf, scoring=\"neg_root_mean_squared_error\", n_jobs=-1\n    )\n    return np.mean(scores)\n\n\ndef objective_histgbm(trial):\n    model = create_histgbm(trial)\n    scores = cross_val_score(\n        model, X, y_log, cv=kf, scoring=\"neg_root_mean_squared_error\", n_jobs=-1\n    )\n    return np.mean(scores)\n\n\ndef objective_knn(trial):\n    model = create_knn(trial)\n    scores = cross_val_score(\n        model, X, y_log, cv=kf, scoring=\"neg_root_mean_squared_error\", n_jobs=-1\n    )\n    return np.mean(scores)\n\n\n# --- Run Optuna Studies ---\nstudy_lgbm = optuna.create_study(direction=\"maximize\")\nstudy_lgbm.optimize(objective_lgbm, n_trials=20)\nlgbm_best_params = study_lgbm.best_params\n\nstudy_xgb = optuna.create_study(direction=\"maximize\")\nstudy_xgb.optimize(objective_xgb, n_trials=20)\nxgb_best_params = study_xgb.best_params\n\nstudy_catboost = optuna.create_study(direction=\"maximize\")\nstudy_catboost.optimize(objective_catboost, n_trials=20)\ncatboost_best_params = study_catboost.best_params\n\nstudy_elasticnet = optuna.create_study(direction=\"maximize\")\nstudy_elasticnet.optimize(objective_elasticnet, n_trials=10)\nelasticnet_best_params = study_elasticnet.best_params\n\nstudy_histgbm = optuna.create_study(direction=\"maximize\")\nstudy_histgbm.optimize(objective_histgbm, n_trials=20)\nhistgbm_best_params = study_histgbm.best_params\n\nstudy_knn = optuna.create_study(direction=\"maximize\")\nstudy_knn.optimize(objective_knn, n_trials=10)\nknn_best_params = study_knn.best_params\n\n# --- OOF and Test Prediction Generation ---\nlgbm_oof = np.zeros(len(train))\nlgbm_test = np.zeros(len(test))\nxgb_oof = np.zeros(len(train))\nxgb_test = np.zeros(len(test))\ncatboost_oof = np.zeros(len(train))\ncatboost_test = np.zeros(len(test))\nelasticnet_oof = np.zeros(len(train))\nelasticnet_test = np.zeros(len(test))\nhistgbm_oof = np.zeros(len(train))\nhistgbm_test = np.zeros(len(test))\nknn_oof = np.zeros(len(train))\nknn_test = np.zeros(len(test))\n\nfor fold, (train_index, val_index) in enumerate(kf.split(X, y_log)):\n    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n    y_train_fold, y_val_fold = y_log.iloc[train_index], y_log.iloc[val_index]\n\n    # Group-based aggregation features (within fold)\n    X_train_fold[\"Neighborhood_MedianQual\"] = X_train_fold[\"Neighborhood\"].map(\n        X_train_fold.groupby(\"Neighborhood\")[\"OverallQual\"].median()\n    )\n    X_val_fold[\"Neighborhood_MedianQual\"] = X_val_fold[\"Neighborhood\"].map(\n        X_train_fold.groupby(\"Neighborhood\")[\"OverallQual\"].median()\n    )\n    X_test[\"Neighborhood_MedianQual\"] = X_test[\"Neighborhood\"].map(\n        X_train_fold.groupby(\"Neighborhood\")[\"OverallQual\"].median()\n    )\n\n    X_train_fold[\"Neighborhood_MeanArea\"] = X_train_fold[\"Neighborhood\"].map(\n        X_train_fold.groupby(\"Neighborhood\")[\"GrLivArea\"].mean()\n    )\n    X_val_fold[\"Neighborhood_MeanArea\"] = X_val_fold[\"Neighborhood\"].map(\n        X_train_fold.groupby(\"Neighborhood\")[\"GrLivArea\"].mean()\n    )\n    X_test[\"Neighborhood_MeanArea\"] = X_test[\"Neighborhood\"].map(\n        X_train_fold.groupby(\"Neighborhood\")[\"GrLivArea\"].mean()\n    )\n\n    X_train_fold[\"Neighborhood_Count\"] = X_train_fold[\"Neighborhood\"].map(\n        X_train_fold[\"Neighborhood\"].value_counts()\n    )\n    X_val_fold[\"Neighborhood_Count\"] = X_val_fold[\"Neighborhood\"].map(\n        X_train_fold[\"Neighborhood\"].value_counts()\n    )\n    X_test[\"Neighborhood_Count\"] = X_test[\"Neighborhood\"].map(\n        X_train_fold[\"Neighborhood\"].value_counts()\n    )\n\n    # Scaling (for KNN)\n    scaler = RobustScaler()\n    X_train_scaled = scaler.fit_transform(X_train_fold)\n    X_val_scaled = scaler.transform(X_val_fold)\n    X_test_scaled = scaler.transform(X_test)\n\n    X_train_scaled = pd.DataFrame(\n        X_train_scaled, index=X_train_fold.index, columns=X_train_fold.columns\n    )\n    X_val_scaled = pd.DataFrame(\n        X_val_scaled, index=X_val_fold.index, columns=X_val_fold.columns\n    )\n    X_test_scaled = pd.DataFrame(\n        X_test_scaled, index=X_test.index, columns=X_test.columns\n    )\n\n    # LightGBM\n    lgbm = lgb.LGBMRegressor(\n        **lgbm_best_params, objective=\"regression\", random_state=42, n_jobs=-1\n    )\n    lgbm.fit(X_train_fold, y_train_fold)\n    lgbm_oof[val_index] = lgbm.predict(X_val_fold)\n    lgbm_test += lgbm.predict(X_test) / n_splits\n\n    # XGBoost\n    xgb_model = xgb.XGBRegressor(\n        **xgb_best_params, objective=\"reg:squarederror\", random_state=42, n_jobs=-1\n    )\n    xgb_model.fit(X_train_fold, y_train_fold)\n    xgb_oof[val_index] = xgb_model.predict(X_val_fold)\n    xgb_test += xgb_model.predict(X_test) / n_splits\n\n    # CatBoost\n    catboost = cb.CatBoostRegressor(**catboost_best_params, random_state=42, verbose=0)\n    catboost.fit(X_train_fold, y_train_fold)\n    catboost_oof[val_index] = catboost.predict(X_val_fold)\n    catboost_test += catboost.predict(X_test) / n_splits\n\n    # ElasticNet\n    elasticnet = ElasticNetCV(**elasticnet_best_params, random_state=42)\n    elasticnet.fit(X_train_fold, y_train_fold)\n    elasticnet_oof[val_index] = elasticnet.predict(X_val_fold)\n    elasticnet_test += elasticnet.predict(X_test) / n_splits\n\n    # HistGradientBoostingRegressor\n    histgbm = HistGradientBoostingRegressor(**histgbm_best_params, random_state=42)\n    histgbm.fit(X_train_fold, y_train_fold)\n    histgbm_oof[val_index] = histgbm.predict(X_val_fold)\n    histgbm_test += histgbm.predict(X_test) / n_splits\n\n    # KNeighborsRegressor\n    knn = KNeighborsRegressor(**knn_best_params)\n    knn.fit(X_train_scaled, y_train_fold)\n    knn_oof[val_index] = knn.predict(X_val_scaled)\n    knn_test += knn.predict(X_test_scaled) / n_splits\n\n# --- Optimized Weighted Blending ---\nOOF_MATRIX = np.column_stack(\n    [lgbm_oof, xgb_oof, catboost_oof, elasticnet_oof, histgbm_oof, knn_oof]\n)\nTEST_MATRIX = np.column_stack(\n    [lgbm_test, xgb_test, catboost_test, elasticnet_test, histgbm_test, knn_test]\n)\n\n\ndef blending_objective(weights):\n    blended_oof = np.dot(OOF_MATRIX, weights)\n    return np.sqrt(mean_squared_error(y_log, blended_oof))\n\n\ninitial_weights = [1 / OOF_MATRIX.shape[1]] * OOF_MATRIX.shape[1]\nbounds = [(0, 1)] * OOF_MATRIX.shape[1]\nconstraints = {\"type\": \"eq\", \"fun\": lambda weights: np.sum(weights) - 1}\n\nresult = minimize(\n    blending_objective,\n    initial_weights,\n    method=\"SLSQP\",\n    bounds=bounds,\n    constraints=constraints,\n)\noptimal_weights = result.x\n\nfinal_predictions_log = np.dot(TEST_MATRIX, optimal_weights)\nfinal_predictions = np.expm1(final_predictions_log)\n\n# --- Submission File Generation ---\nsubmission = pd.DataFrame({\"Id\": test_ID, \"SalePrice\": final_predictions})\nsubmission.to_csv(\"working/submission.csv\", index=False)\n\n# --- Evaluation ---\nblended_oof = np.dot(OOF_MATRIX, optimal_weights)\nrmse_blended = np.sqrt(mean_squared_error(y_log, blended_oof))\nprint(f\"RMSE on blended OOF predictions: {rmse_blended}\")", "import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom scipy.special import boxcox1p\nfrom scipy.stats import skew\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV\nfrom scipy.optimize import minimize\n\n# 1. Load & Combine\ntrain = pd.read_csv(\"input/train.csv\")\ntest = pd.read_csv(\"input/test.csv\")\ntest_ID = test[\"Id\"]\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop([\"Id\"], axis=1, inplace=True)\n\n# 2. Target Transform & Outliers\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nall_data[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nall_data = all_data[all_data[\"GrLivArea\"] < 4000]\nall_data.reset_index(drop=True, inplace=True)\n\n# 3. Missing Value Imputation\nfor col in [\n    \"PoolQC\",\n    \"MiscFeature\",\n    \"Alley\",\n    \"Fence\",\n    \"FireplaceQu\",\n    \"GarageType\",\n    \"GarageFinish\",\n    \"GarageQual\",\n    \"GarageCond\",\n    \"BsmtQual\",\n    \"BsmtCond\",\n    \"BsmtExposure\",\n    \"BsmtFinType1\",\n    \"BsmtFinType2\",\n    \"MasVnrType\",\n]:\n    all_data[col] = all_data[col].fillna(\"None\")\n\nfor col in [\n    \"GarageYrBlt\",\n    \"GarageArea\",\n    \"GarageCars\",\n    \"BsmtFinSF1\",\n    \"BsmtFinSF2\",\n    \"BsmtUnfSF\",\n    \"TotalBsmtSF\",\n    \"BsmtFullBath\",\n    \"BsmtHalfBath\",\n    \"MasVnrArea\",\n]:\n    all_data[col] = all_data[col].fillna(0)\n\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median())\n)\n\nall_data[\"MSZoning\"] = all_data[\"MSZoning\"].fillna(all_data[\"MSZoning\"].mode()[0])\nall_data[\"Electrical\"] = all_data[\"Electrical\"].fillna(all_data[\"Electrical\"].mode()[0])\nall_data[\"KitchenQual\"] = all_data[\"KitchenQual\"].fillna(\n    all_data[\"KitchenQual\"].mode()[0]\n)\nall_data[\"Exterior1st\"] = all_data[\"Exterior1st\"].fillna(\n    all_data[\"Exterior1st\"].mode()[0]\n)\nall_data[\"Exterior2nd\"] = all_data[\"Exterior2nd\"].fillna(\n    all_data[\"Exterior2nd\"].mode()[0]\n)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(all_data[\"Functional\"].mode()[0])\nall_data[\"SaleType\"] = all_data[\"SaleType\"].fillna(all_data[\"SaleType\"].mode()[0])\nall_data[\"Utilities\"] = all_data[\"Utilities\"].fillna(all_data[\"Utilities\"].mode()[0])\n\n# 4. Advanced Feature Engineering\n# Interaction Features\nall_data[\"TotalSF\"] = (\n    all_data[\"TotalBsmtSF\"] + all_data[\"1stFlrSF\"] + all_data[\"2ndFlrSF\"]\n)\nall_data[\"OverallQual_x_TotalSF\"] = all_data[\"OverallQual\"] * all_data[\"TotalSF\"]\nall_data[\"YearBuilt_x_GarageCars\"] = all_data[\"YearBuilt\"] * all_data[\"GarageCars\"]\n\n# Neighborhood Aggregations\nfor col in [\"GrLivArea\", \"OverallQual\", \"TotalSF\", \"GarageArea\"]:\n    all_data[f\"Neighborhood_Median_{col}\"] = all_data.groupby(\"Neighborhood\")[\n        col\n    ].transform(\"median\")\n    all_data[f\"Neighborhood_Mean_{col}\"] = all_data.groupby(\"Neighborhood\")[\n        col\n    ].transform(\"mean\")\n    all_data[f\"Neighborhood_Std_{col}\"] = all_data.groupby(\"Neighborhood\")[\n        col\n    ].transform(\"std\")\n\n# Label Encoding\nquality_map = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"None\": 0}\nfor col in [\n    \"ExterQual\",\n    \"ExterCond\",\n    \"BsmtQual\",\n    \"BsmtCond\",\n    \"HeatingQC\",\n    \"KitchenQual\",\n    \"FireplaceQu\",\n    \"GarageQual\",\n    \"GarageCond\",\n    \"PoolQC\",\n]:\n    all_data[col] = all_data[col].map(quality_map)\n\nexposure_map = {\"Gd\": 4, \"Av\": 3, \"Mn\": 2, \"No\": 1, \"None\": 0}\nall_data[\"BsmtExposure\"] = all_data[\"BsmtExposure\"].map(exposure_map)\n\nfinish_map = {\"GLQ\": 5, \"ALQ\": 4, \"BLQ\": 3, \"Rec\": 2, \"LwQ\": 1, \"Unf\": 0, \"None\": 0}\nfor col in [\"BsmtFinType1\", \"BsmtFinType2\"]:\n    all_data[col] = all_data[col].map(finish_map)\n\nfunctional_map = {\n    \"Typ\": 7,\n    \"Min1\": 6,\n    \"Min2\": 5,\n    \"Mod\": 4,\n    \"Maj1\": 3,\n    \"Maj2\": 2,\n    \"Sev\": 1,\n    \"Sal\": 0,\n}\nall_data[\"Functional\"] = all_data[\"Functional\"].map(functional_map)\n\nfence_map = {\"GdPrv\": 4, \"MnPrv\": 3, \"GdWo\": 2, \"MnWw\": 1, \"None\": 0}\nall_data[\"Fence\"] = all_data[\"Fence\"].map(fence_map)\n\n# Feature Simplification and Transformation\nfor col in [\"MSSubClass\", \"OverallCond\", \"YrSold\", \"MoSold\"]:\n    all_data[col] = all_data[col].astype(str)\n\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\nskewed_feats = skewed_feats[skewed_feats > 0.5]\nskewed_feats = skewed_feats.index\n\nfor feat in skewed_feats:\n    all_data[feat] = boxcox1p(all_data[feat], 0.15)\n\n# Final Encoding\nall_data = pd.get_dummies(all_data)\n\n# Split back into train and test\nX = all_data[: len(train)]\nX_test = all_data[len(train) :]\ny_log = train[\"SalePrice\"]\n\n# Feature Selection\nselector = SelectFromModel(LassoCV(cv=5))\nselector.fit(X, y_log)\nX = X.loc[:, selector.get_support()]\nX_test = X_test.loc[:, selector.get_support()]\n\n# Scaling\nscaler = RobustScaler()\nX_scaled = scaler.fit_transform(X)\nX_test_scaled = scaler.transform(X_test)\n\nX_scaled = pd.DataFrame(X_scaled, columns=X.columns)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n\nX = pd.DataFrame(X, columns=X.columns)\nX_test = pd.DataFrame(X_test, columns=X_test.columns)\n\n# Model Training\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# Initialize OOF and test prediction arrays\noof_lgbm = np.zeros(X.shape[0])\ntest_lgbm = np.zeros(X_test.shape[0])\noof_xgb = np.zeros(X.shape[0])\ntest_xgb = np.zeros(X_test.shape[0])\noof_elastic = np.zeros(X.shape[0])\ntest_elastic = np.zeros(X_test.shape[0])\noof_hgb = np.zeros(X.shape[0])\ntest_hgb = np.zeros(X_test.shape[0])\noof_mlp = np.zeros(X.shape[0])\ntest_mlp = np.zeros(X_test.shape[0])\n\n# Model parameters\nlgbm_params = {\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"n_estimators\": 2000,\n    \"learning_rate\": 0.01,\n    \"num_leaves\": 31,\n    \"feature_fraction\": 0.8,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 1,\n    \"min_child_samples\": 10,\n    \"random_state\": 42,\n    \"n_jobs\": -1,\n}\n\nxgb_params = {\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"n_estimators\": 2000,\n    \"learning_rate\": 0.01,\n    \"max_depth\": 5,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"random_state\": 42,\n    \"n_jobs\": -1,\n    \"early_stopping_rounds\": 50,\n}\n\nelastic_params = {\n    \"l1_ratio\": [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],\n    \"n_alphas\": 100,\n    \"cv\": 5,\n    \"max_iter\": 5000,\n    \"random_state\": 42,\n    \"n_jobs\": -1,\n}\n\nhgb_params = {\n    \"learning_rate\": 0.05,\n    \"max_iter\": 500,\n    \"max_leaf_nodes\": 31,\n    \"validation_fraction\": 0.1,\n    \"n_iter_no_change\": 20,\n    \"random_state\": 42,\n}\n\nmlp_params = {\n    \"hidden_layer_sizes\": (64, 32),\n    \"activation\": \"relu\",\n    \"solver\": \"adam\",\n    \"alpha\": 0.001,\n    \"batch_size\": 32,\n    \"learning_rate_init\": 0.001,\n    \"early_stopping\": True,\n    \"n_iter_no_change\": 15,\n    \"random_state\": 42,\n    \"max_iter\": 500,\n}\n\n# CV Loop\nfor fold, (train_index, val_index) in enumerate(kf.split(X, y_log)):\n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    X_train_scaled, X_val_scaled = X_scaled.iloc[train_index], X_scaled.iloc[val_index]\n    y_train, y_val = y_log.iloc[train_index], y_log.iloc[val_index]\n\n    # LightGBM\n    model_lgbm = lgb.LGBMRegressor(**lgbm_params)\n    model_lgbm.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_val, y_val)],\n        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)],\n    )\n    oof_lgbm[val_index] = model_lgbm.predict(X_val)\n    test_lgbm += model_lgbm.predict(X_test) / kf.n_splits\n\n    # XGBoost\n    model_xgb = xgb.XGBRegressor(**xgb_params)\n    model_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    oof_xgb[val_index] = model_xgb.predict(X_val)\n    test_xgb += model_xgb.predict(X_test) / kf.n_splits\n\n    # ElasticNet\n    model_elastic = ElasticNetCV(**elastic_params)\n    model_elastic.fit(X_train_scaled, y_train)\n    oof_elastic[val_index] = model_elastic.predict(X_val_scaled)\n    test_elastic += model_elastic.predict(X_test_scaled) / kf.n_splits\n\n    # HistGradientBoosting\n    model_hgb = HistGradientBoostingRegressor(**hgb_params)\n    model_hgb.fit(X_train, y_train)\n    oof_hgb[val_index] = model_hgb.predict(X_val)\n    test_hgb += model_hgb.predict(X_test) / kf.n_splits\n\n    # MLP\n    model_mlp = MLPRegressor(**mlp_params)\n    model_mlp.fit(X_train_scaled, y_train)\n    oof_mlp[val_index] = model_mlp.predict(X_val_scaled)\n    test_mlp += model_mlp.predict(X_test_scaled) / kf.n_splits\n\n\n# Weight Optimization\ndef objective(weights):\n    ensemble_oof = (\n        weights[0] * oof_lgbm\n        + weights[1] * oof_xgb\n        + weights[2] * oof_elastic\n        + weights[3] * oof_hgb\n        + weights[4] * oof_mlp\n    )\n    return np.sqrt(mean_squared_error(y_log, ensemble_oof))\n\n\n# Initial guess, bounds, and constraints\ninitial_guess = [0.2, 0.2, 0.2, 0.2, 0.2]\nbounds = [(0, 1)] * 5\nconstraints = {\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1}\n\n# Optimization\nresult = minimize(\n    objective, initial_guess, method=\"SLSQP\", bounds=bounds, constraints=constraints\n)\noptimal_weights = result.x\n\n# Final Prediction\nfinal_test_pred = (\n    optimal_weights[0] * test_lgbm\n    + optimal_weights[1] * test_xgb\n    + optimal_weights[2] * test_elastic\n    + optimal_weights[3] * test_hgb\n    + optimal_weights[4] * test_mlp\n)\n\n# Inverse Transform\nfinal_test_pred = np.expm1(final_test_pred)\n\n# Create Submission File\nsubmission = pd.DataFrame({\"Id\": test_ID, \"SalePrice\": final_test_pred})\nsubmission.to_csv(\"working/submission.csv\", index=False)\n\n# Local Validation\nfinal_oof = (\n    optimal_weights[0] * oof_lgbm\n    + optimal_weights[1] * oof_xgb\n    + optimal_weights[2] * oof_elastic\n    + optimal_weights[3] * oof_hgb\n    + optimal_weights[4] * oof_mlp\n)\nrmse_oof = np.sqrt(mean_squared_error(y_log, final_oof))\nprint(f\"Optimized OOF RMSE: {rmse_oof}\")", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom sklearn.linear_model import ElasticNetCV\nimport lightgbm as lgb\nimport xgboost as xgb\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.special import boxcox1p\nfrom scipy.optimize import minimize\nimport optuna\nimport os\n\n# Load data\ntrain = pd.read_csv(\"input/train.csv\")\ntest = pd.read_csv(\"input/test.csv\")\ntest_ID = test[\"Id\"]\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(\"Id\", axis=1, inplace=True)\n\n# Target transformation\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny_log = train[\"SalePrice\"]\nall_data = all_data.iloc[: len(train) + len(test)]\n\n# Outlier removal\nall_data = all_data[~((all_data[\"GrLivArea\"] > 4000) & (all_data[\"SalePrice\"] < 12.5))]\ny_log = y_log[all_data.index[: len(train)]]\n\n# Combine data for preprocessing\nntrain = train.shape[0]\nntest = test.shape[0]\nall_data = pd.concat((all_data[:ntrain], all_data[ntrain:])).reset_index(drop=True)\n\n# Missing value imputation\nfor col in [\n    \"PoolQC\",\n    \"Alley\",\n    \"Fence\",\n    \"FireplaceQu\",\n    \"GarageType\",\n    \"GarageFinish\",\n    \"GarageQual\",\n    \"GarageCond\",\n    \"BsmtQual\",\n    \"BsmtCond\",\n    \"BsmtExposure\",\n    \"BsmtFinType1\",\n    \"BsmtFinType2\",\n    \"MasVnrType\",\n    \"MiscFeature\",\n]:\n    all_data[col] = all_data[col].fillna(\"None\")\n\nfor col in [\n    \"GarageYrBlt\",\n    \"GarageArea\",\n    \"GarageCars\",\n    \"BsmtFinSF1\",\n    \"BsmtFinSF2\",\n    \"BsmtUnfSF\",\n    \"TotalBsmtSF\",\n    \"BsmtFullBath\",\n    \"BsmtHalfBath\",\n    \"MasVnrArea\",\n]:\n    all_data[col] = all_data[col].fillna(0)\n\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median())\n)\n\nfor col in [\n    \"MSZoning\",\n    \"Electrical\",\n    \"KitchenQual\",\n    \"Exterior1st\",\n    \"Exterior2nd\",\n    \"SaleType\",\n    \"Functional\",\n    \"Utilities\",\n]:\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n\n# Feature Engineering\nall_data[\"TotalSF\"] = (\n    all_data[\"TotalBsmtSF\"] + all_data[\"1stFlrSF\"] + all_data[\"2ndFlrSF\"]\n)\nall_data[\"TotalBath\"] = (\n    all_data[\"FullBath\"]\n    + (0.5 * all_data[\"HalfBath\"])\n    + all_data[\"BsmtFullBath\"]\n    + (0.5 * all_data[\"BsmtHalfBath\"])\n)\nall_data[\"TotalPorchSF\"] = (\n    all_data[\"OpenPorchSF\"]\n    + all_data[\"EnclosedPorch\"]\n    + all_data[\"3SsnPorch\"]\n    + all_data[\"ScreenPorch\"]\n)\nall_data[\"HasPool\"] = all_data[\"PoolArea\"].apply(lambda x: 1 if x > 0 else 0)\nall_data[\"HasGarage\"] = all_data[\"GarageArea\"].apply(lambda x: 1 if x > 0 else 0)\nall_data[\"HasBsmt\"] = all_data[\"TotalBsmtSF\"].apply(lambda x: 1 if x > 0 else 0)\nall_data[\"IsNewHouse\"] = (all_data[\"YearBuilt\"] == all_data[\"YrSold\"]).astype(int)\n\n# Polynomial Features\nimportant_numerical_cols = [\n    \"OverallQual\",\n    \"GrLivArea\",\n    \"TotalSF\",\n    \"GarageCars\",\n    \"YearBuilt\",\n]\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\npoly_features = poly.fit_transform(all_data[important_numerical_cols])\npoly_df = pd.DataFrame(\n    poly_features, columns=[f\"poly_{i}\" for i in range(poly_features.shape[1])]\n)\nall_data = pd.concat([all_data, poly_df], axis=1)\n\n# Interaction Features\nall_data[\"OverallQual_x_TotalSF\"] = all_data[\"OverallQual\"] * all_data[\"TotalSF\"]\nall_data[\"GrLivArea_x_OverallQual\"] = all_data[\"GrLivArea\"] * all_data[\"OverallQual\"]\n\n# Type Conversion\nfor col in [\"MSSubClass\", \"OverallCond\", \"YrSold\", \"MoSold\"]:\n    all_data[col] = all_data[col].astype(str)\n\n# Skew Correction\nnumerical_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = all_data[numerical_feats].apply(lambda x: x.skew())\nskewed_feats = skewed_feats[abs(skewed_feats) > 0.5]\nskewed_feats = skewed_feats.index\n\nfor feat in skewed_feats:\n    all_data[feat] = boxcox1p(all_data[feat], 0.15)\n\n# Ordinal Mapping\nquality_map = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"None\": 0}\nfor col in [\n    \"ExterQual\",\n    \"ExterCond\",\n    \"HeatingQC\",\n    \"KitchenQual\",\n    \"FireplaceQu\",\n    \"GarageQual\",\n    \"GarageCond\",\n    \"PoolQC\",\n    \"BsmtQual\",\n    \"BsmtCond\",\n]:\n    all_data[col] = all_data[col].map(quality_map)\n\nexposure_map = {\"Gd\": 4, \"Av\": 3, \"Mn\": 2, \"No\": 1, \"None\": 0}\nall_data[\"BsmtExposure\"] = all_data[\"BsmtExposure\"].map(exposure_map)\n\nfinish_map = {\"Fin\": 3, \"GLQ\": 3, \"ALQ\": 2, \"Rec\": 2, \"BLQ\": 1, \"Unf\": 1, \"None\": 0}\nfor col in [\"BsmtFinType1\", \"BsmtFinType2\"]:\n    all_data[col] = all_data[col].map(finish_map)\n\nfence_map = {\"GdPrv\": 4, \"MnPrv\": 3, \"GdWo\": 2, \"MnWw\": 1, \"None\": 0}\nall_data[\"Fence\"] = all_data[\"Fence\"].map(fence_map)\n\nshape_map = {\"Reg\": 3, \"IR1\": 2, \"IR2\": 1, \"IR3\": 0}\nall_data[\"LotShape\"] = all_data[\"LotShape\"].map(shape_map)\n\nlandcontour_map = {\"Lvl\": 3, \"Bnk\": 2, \"HLS\": 1, \"Low\": 0}\nall_data[\"LandContour\"] = all_data[\"LandContour\"].map(landcontour_map)\n\nslope_map = {\"Gtl\": 2, \"Mod\": 1, \"Sev\": 0}\nall_data[\"LandSlope\"] = all_data[\"LandSlope\"].map(slope_map)\n\n# One-Hot Encoding\nall_data = pd.get_dummies(all_data)\n\n# Split data\nX = all_data[:ntrain]\nX_test = all_data[ntrain:]\n\n\n# PyTorch MLP Model\nclass HousePriceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X.values, dtype=torch.float32)\n        self.y = torch.tensor(y.values, dtype=torch.float32).reshape(-1, 1)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_features):\n        super(MLP, self).__init__()\n        self.batchnorm1d = nn.BatchNorm1d(in_features)\n        self.linear1 = nn.Linear(in_features, 128)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.3)\n        self.linear2 = nn.Linear(128, 64)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.3)\n        self.linear3 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = self.batchnorm1d(x)\n        x = self.linear1(x)\n        x = self.relu1(x)\n        x = self.dropout1(x)\n        x = self.linear2(x)\n        x = self.relu2(x)\n        x = self.dropout2(x)\n        x = self.linear3(x)\n        return x\n\n\n# Model Training\nn_splits = 10\nn_repeats = 3\nrkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n\n# Initialize arrays for OOF and test predictions\noof_lgb = np.zeros(ntrain)\ntest_lgb = np.zeros(ntest)\noof_xgb = np.zeros(ntrain)\ntest_xgb = np.zeros(ntest)\noof_elastic = np.zeros(ntrain)\ntest_elastic = np.zeros(ntest)\noof_mlp = np.zeros(ntrain)\ntest_mlp = np.zeros(ntest)\n\n\n# Optuna Hyperparameter Optimization\ndef objective_lgb(trial):\n    lgb_params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"random_state\": 42,\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 5000),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 5e-2),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 100),\n        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-2, 10.0),\n        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-2, 10.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n    }\n\n    cv_scores = []\n    for train_index, val_index in RepeatedKFold(\n        n_splits=5, n_repeats=1, random_state=42\n    ).split(X, y_log):\n        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n        y_train, y_val = y_log.iloc[train_index], y_log.iloc[val_index]\n\n        model = lgb.LGBMRegressor(**lgb_params)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_val, y_val)],\n            early_stopping_rounds=100,\n            verbose=False,\n        )\n        preds = model.predict(X_val)\n        rmse = np.sqrt(mean_squared_error(y_val, preds))\n        cv_scores.append(rmse)\n\n    return np.mean(cv_scores)\n\n\ndef objective_xgb(trial):\n    xgb_params = {\n        \"objective\": \"reg:squarederror\",\n        \"eval_metric\": \"rmse\",\n        \"random_state\": 42,\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 5000),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 5e-2),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-2, 10.0),\n        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-2, 10.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n    }\n\n    cv_scores = []\n    for train_index, val_index in RepeatedKFold(\n        n_splits=5, n_repeats=1, random_state=42\n    ).split(X, y_log):\n        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n        y_train, y_val = y_log.iloc[train_index], y_log.iloc[val_index]\n\n        model = xgb.XGBRegressor(**xgb_params, early_stopping_rounds=100)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n        preds = model.predict(X_val)\n        rmse = np.sqrt(mean_squared_error(y_val, preds))\n        cv_scores.append(rmse)\n\n    return np.mean(cv_scores)\n\n\ndef objective_mlp(trial):\n    mlp_params = {\n        \"lr\": trial.suggest_loguniform(\"lr\", 1e-4, 1e-2),\n        \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3),\n        \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128, 256]),\n    }\n\n    cv_scores = []\n    for train_index, val_index in RepeatedKFold(\n        n_splits=5, n_repeats=1, random_state=42\n    ).split(X, y_log):\n        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n        y_train, y_val = y_log.iloc[train_index], y_log.iloc[val_index]\n\n        train_dataset = HousePriceDataset(X_train, y_train)\n        val_dataset = HousePriceDataset(X_val, y_val)\n        train_loader = DataLoader(\n            train_dataset, batch_size=int(mlp_params[\"batch_size\"]), shuffle=True\n        )\n        val_loader = DataLoader(\n            val_dataset, batch_size=int(mlp_params[\"batch_size\"]), shuffle=False\n        )\n\n        model = MLP(X_train.shape[1])\n        optimizer = optim.AdamW(\n            model.parameters(),\n            lr=mlp_params[\"lr\"],\n            weight_decay=mlp_params[\"weight_decay\"],\n        )\n        criterion = nn.MSELoss()\n\n        best_rmse = float(\"inf\")\n        patience = 50\n        counter = 0\n\n        for epoch in range(200):\n            model.train()\n            for X_batch, y_batch in train_loader:\n                optimizer.zero_grad()\n                outputs = model(X_batch)\n                loss = criterion(outputs, y_batch)\n                loss.backward()\n                optimizer.step()\n\n            model.eval()\n            val_preds = []\n            val_targets = []\n            with torch.no_grad():\n                for X_batch, y_batch in val_loader:\n                    outputs = model(X_batch)\n                    val_preds.extend(outputs.cpu().numpy().flatten())\n                    val_targets.extend(y_batch.cpu().numpy().flatten())\n\n            rmse = np.sqrt(mean_squared_error(val_targets, val_preds))\n\n            if rmse < best_rmse:\n                best_rmse = rmse\n                counter = 0\n            else:\n                counter += 1\n                if counter >= patience:\n                    break\n\n        cv_scores.append(best_rmse)\n\n    return np.mean(cv_scores)\n\n\n# Run Optuna studies\nstudy_lgb = optuna.create_study(direction=\"minimize\")\nstudy_lgb.optimize(objective_lgb, n_trials=25)\nbest_params_lgb = study_lgb.best_params\n\nstudy_xgb = optuna.create_study(direction=\"minimize\")\nstudy_xgb.optimize(objective_xgb, n_trials=25)\nbest_params_xgb = study_xgb.best_params\n\nstudy_mlp = optuna.create_study(direction=\"minimize\")\nstudy_mlp.optimize(objective_mlp, n_trials=25)\nbest_params_mlp = study_mlp.best_params\n\n# Cross-validation training loop\nfor fold, (train_index, val_index) in enumerate(rkf.split(X, y_log)):\n    print(f\"Fold {fold+1}\")\n    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_val = y_log.iloc[train_index], y_log.iloc[val_index]\n    X_test_fold = X_test.copy()\n\n    # Neighborhood Aggregations (within CV loop)\n    for col in [\"GrLivArea\", \"SalePrice\"]:\n        if col in X_train.columns:\n            median_by_neighborhood = X_train.groupby(\"Neighborhood\")[col].median()\n            X_train[f\"Neighborhood_Med{col}\"] = X_train[\"Neighborhood\"].map(\n                median_by_neighborhood\n            )\n            X_val[f\"Neighborhood_Med{col}\"] = X_val[\"Neighborhood\"].map(\n                median_by_neighborhood\n            )\n            X_test_fold[f\"Neighborhood_Med{col}\"] = X_test_fold[\"Neighborhood\"].map(\n                median_by_neighborhood\n            )\n\n    if \"SalePrice\" in X_train.columns:\n        price_ratio = (\n            X_train.groupby(\"Neighborhood\")[\"OverallQual\"].median()\n            / X_train.groupby(\"Neighborhood\")[\"SalePrice\"].median()\n        )\n        X_train[f\"Neighborhood_Price_Ratio\"] = X_train[\"Neighborhood\"].map(price_ratio)\n        X_val[f\"Neighborhood_Price_Ratio\"] = X_val[\"Neighborhood\"].map(price_ratio)\n        X_test_fold[f\"Neighborhood_Price_Ratio\"] = X_test_fold[\"Neighborhood\"].map(\n            price_ratio\n        )\n\n    # Scaling\n    scaler = RobustScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n    X_test_fold = scaler.transform(X_test_fold)\n\n    X_train = pd.DataFrame(X_train, columns=X.columns)\n    X_val = pd.DataFrame(X_val, columns=X.columns)\n    X_test_fold = pd.DataFrame(X_test_fold, columns=X.columns)\n\n    # LightGBM\n    lgb_model = lgb.LGBMRegressor(\n        **best_params_lgb, objective=\"regression\", metric=\"rmse\", random_state=42\n    )\n    lgb_model.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_val, y_val)],\n        callbacks=[lgb.early_stopping(100, verbose=False)],\n    )\n    oof_lgb[val_index] += lgb_model.predict(X_val) / n_repeats\n    test_lgb += lgb_model.predict(X_test_fold) / (n_splits * n_repeats)\n\n    # XGBoost\n    xgb_model = xgb.XGBRegressor(\n        **best_params_xgb,\n        objective=\"reg:squarederror\",\n        eval_metric=\"rmse\",\n        random_state=42,\n        early_stopping_rounds=100,\n    )\n    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n    oof_xgb[val_index] += xgb_model.predict(X_val) / n_repeats\n    test_xgb += xgb_model.predict(X_test_fold) / (n_splits * n_repeats)\n\n    # ElasticNet\n    elastic_model = ElasticNetCV(\n        l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 0.995, 1],\n        eps=0.001,\n        n_alphas=100,\n        cv=5,\n        random_state=42,\n    )\n    elastic_model.fit(X_train, y_train)\n    oof_elastic[val_index] += elastic_model.predict(X_val) / n_repeats\n    test_elastic += elastic_model.predict(X_test_fold) / (n_splits * n_repeats)\n\n    # MLP\n    train_dataset = HousePriceDataset(X_train, y_train)\n    val_dataset = HousePriceDataset(X_val, y_val)\n    test_dataset = HousePriceDataset(X_test_fold, np.zeros(X_test_fold.shape[0]))\n    train_loader = DataLoader(\n        train_dataset, batch_size=int(best_params_mlp[\"batch_size\"]), shuffle=True\n    )\n    val_loader = DataLoader(\n        val_dataset, batch_size=int(best_params_mlp[\"batch_size\"]), shuffle=False\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=int(best_params_mlp[\"batch_size\"]), shuffle=False\n    )\n\n    mlp_model = MLP(X_train.shape[1])\n    optimizer = optim.AdamW(\n        mlp_model.parameters(),\n        lr=best_params_mlp[\"lr\"],\n        weight_decay=best_params_mlp[\"weight_decay\"],\n    )\n    criterion = nn.MSELoss()\n\n    best_rmse = float(\"inf\")\n    patience = 50\n    counter = 0\n\n    for epoch in range(200):\n        mlp_model.train()\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = mlp_model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n\n        mlp_model.eval()\n        val_preds = []\n        val_targets = []\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                outputs = mlp_model(X_batch)\n                val_preds.extend(outputs.cpu().numpy().flatten())\n                val_targets.extend(y_batch.cpu().numpy().flatten())\n\n        rmse = np.sqrt(mean_squared_error(val_targets, val_preds))\n\n        if rmse < best_rmse:\n            best_rmse = rmse\n            counter = 0\n        else:\n            counter += 1\n            if counter >= patience:\n                break\n\n    oof_mlp[val_index] += np.array(val_preds) / n_repeats\n\n    test_preds = []\n    with torch.no_grad():\n        for X_batch, _ in test_loader:\n            outputs = mlp_model(X_batch)\n            test_preds.extend(outputs.cpu().numpy().flatten())\n    test_mlp += np.array(test_preds) / (n_splits * n_repeats)\n\n# Ensemble Weight Optimization\nOOF_matrix = np.column_stack((oof_lgb, oof_xgb, oof_elastic, oof_mlp))\n\n\ndef objective(weights):\n    final_oof_preds = np.dot(OOF_matrix, weights)\n    rmse = np.sqrt(mean_squared_error(y_log, final_oof_preds))\n    return rmse\n\n\n# Constraints for the optimizer\nconstraints = {\"type\": \"eq\", \"fun\": lambda x: np.sum(x) - 1}\nbounds = [(0, 1)] * 4\n\n# Initial guess for weights\ninitial_weights = [0.25, 0.25, 0.25, 0.25]\n\n# Run the optimizer\nresult = minimize(\n    objective, initial_weights, method=\"SLSQP\", bounds=bounds, constraints=constraints\n)\noptimized_weights = result.x\n\n# Final Prediction\nfinal_test_preds = (\n    optimized_weights[0] * test_lgb\n    + optimized_weights[1] * test_xgb\n    + optimized_weights[2] * test_elastic\n    + optimized_weights[3] * test_mlp\n)\n\n# Inverse transform\nfinal_prices = np.expm1(final_test_preds)\n\n# Create submission file\nsubmission = pd.DataFrame({\"Id\": test_ID, \"SalePrice\": final_prices})\nsubmission.to_csv(\"working/submission.csv\", index=False)\n\n# Calculate final OOF RMSE\nfinal_oof_preds = (\n    optimized_weights[0] * oof_lgb\n    + optimized_weights[1] * oof_xgb\n    + optimized_weights[2] * oof_elastic\n    + optimized_weights[3] * oof_mlp\n)\nfinal_rmse = np.sqrt(mean_squared_error(y_log, final_oof_preds))\nprint(f\"Final OOF RMSE: {final_rmse}\")"], "term_out": ["[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002348 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3907\n[LightGBM] [Info] Number of data points in the train set: 1310, number of used features: 195\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 12.007627\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with pos\n ... [26232 characters truncated] ... \nightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nRMSE on OOF Predictions: 0.10802925367844375\nExecution time: a minute seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\nanops.py\", line 785, in nanmedian\n    values = values.astype(\"f8\")\n             ^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'RL'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\opnin\\OneDrive\\Desktop\\aide\\aideml\\aide\\interpreter.py\", line 147, in _run_session\n    exec(compile(code, self.agent_file_name, \"exec\"), global_scope)\n  File \"runfile.py\", line 66, in <module>\n    all_data = all_data.fillna(all_data.median())\n                               ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\", line 11348, in median\n    result = super().median(axis, skipna, numeric_only, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py\", line 12003, in median\n    return self._stat_function(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py\", line 11949, in _stat_function\n    return self._reduce(\n           ^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\", line 11204, in _reduce\n    res = df._mgr.reduce(blk_func)\n          ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 1459, in reduce\n    nbs = blk.reduce(func)\n          ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 377, in reduce\n    result = func(self.values)\n             ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\", line 11136, in blk_func\n    return op(values, axis=axis, skipna=skipna, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\nanops.py\", line 147, in f\n    result = alt(values, axis=axis, skipna=skipna, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\nanops.py\", line 788, in nanmedian\n    raise TypeError(str(err)) from err\nTypeError: could not convert string to float: 'RL'\nExecution time: 2 seconds seconds (time limit is an hour).", "[I 2026-01-11 07:05:29,135] A new study created in memory with name: no-name-c4771be7-8daf-49d3-855d-39e4e767c428\n[I 2026-01-11 07:05:36,230] Trial 0 finished with value: -0.12661907622755503 and parameters: {'n_estimators': 222, 'learning_rate': 0.04282345647849909, 'num_leaves': 38, 'max_depth': 4, 'min_child_samples': 6, 'subsample': 0.7104293059352105, 'colsample_bytree': 0.858633804712097, 'reg_alpha': 0.636131307266012, 'reg_lambda': 0.23439117539965493}. Best is trial 0 with value: -0.12661907622755503.\n[I 2026-01-11 07:05:42,617] Trial 1 finished with value: -0.12759091482145965 and parameters: {'n_estimators': 643, 'learning_rate': 0.09402173914706571, 'num_leaves': 37, 'max_depth': 5, 'min_child_samples': 29, 'subsample': 0.8249163744092043, 'colsample_bytree': 0.7924251128335479, 'reg_alpha': 0.04414070127649916, 'reg_lambda': 0.4667108271749131}. Best is trial 0 with value: -0.12661907622755503.\n[I 2026-01-11 07:05:48,985] Trial 2 finished with value: -0.12553952963932766 and parameters: {'n_estimators': 981, 'learning_rate': 0.04266672411273578, 'num_leaves': 28, 'max_depth': 4, 'min_child_samples': 49, 'subsample': 0.9769437463308356, 'colsample_bytree': 0.8971824012197417, 'reg_alpha': 0.7055481535675093, 'reg_lambda': 0.756464854576709}. Best is trial 2 with value: -0.12553952963932766.\n[I 2026-01-11 07:05:50,233] Trial 3 finished with value: -0.1257540775403613 and parameters: {'n_estimators': 244, 'learning_rate': 0.08271231921563933, 'num_leaves': 22, 'max_depth': 4, 'min_child_samples': 49, 'subsample': 0.9128853079591741, 'colsample_bytree': 0.8426107758013555, 'reg_alpha': 0.7466288937005299, 'reg_lambda': 0.7943911208511883}. Best is trial 2 with value: -0.12553952963932766.\n[I 2026-01-11 07:05:52,374] Trial 4 finished with value: -0.12238774345674075 and parameters: {'n_estimators': 522, 'learning_rate': 0.0750057024438876, 'num_leaves': 35, 'max_depth': 3, 'min_child_samples': 36, 'subsample': 0.8520686692397162, 'colsample_bytree': 0.8324631924727411, 'reg_alpha': 0.4866971397086015, 'reg_lambda': 0.3171473563353161}. Best is trial 4 with value: -0.12238774345674075.\n[I 2026-01-11 07:05:57,589] Trial 5 finished with value: -0.12703687756167917 and parameters: {'n_estimators': 864, 'learning_rate': 0.06546758046267143, 'num_leaves': 27, 'max_depth': 9, 'min_child_samples': 40, 'subsample': 0.8203739324400449, 'colsample_bytree': 0.9621289342745709, 'reg_alpha': 0.7606195143921333, 'reg_lambda': 0.23089374804417606}. Best is trial 4\n ... [13985 characters truncated] ... \nection\\_validation.py\", line 443, in cross_validate\n    _warn_or_raise_about_fit_failures(results, error_score)\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 529, in _warn_or_raise_about_fit_failures\n    raise ValueError(all_fits_failed_message)\nValueError: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1368, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py\", line 200, in train\n    if cb_container.after_iteration(bst, i, dtrain, evals):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\callback.py\", line 269, in after_iteration\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\callback.py\", line 269, in <genexpr>\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\callback.py\", line 461, in after_iteration\n    raise ValueError(msg)\nValueError: Must have at least 1 validation dataset for early stopping.\n\nExecution time: a minute seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"C:\\Users\\opnin\\OneDrive\\Desktop\\aide\\aideml\\aide\\interpreter.py\", line 147, in _run_session\n    exec(compile(code, self.agent_file_name, \"exec\"), global_scope)\n  File \"runfile.py\", line 163, in <module>\n    selector.fit(X, y_log)\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\", line 388, in fit\n    self.estimator_.fit(X, y, **fit_params)\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py\", line 1641, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 645, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_X_params)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1059, in check_array\n    _assert_all_finite(\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 126, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 175, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLassoCV does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\nExecution time: a moment seconds (time limit is an hour).", "[I 2026-01-11 07:09:34,933] A new study created in memory with name: no-name-4c8acaa4-8e56-4817-8480-25825e0e77b7\n[W 2026-01-11 07:09:34,938] Trial 0 failed with parameters: {'n_estimators': 4093, 'learning_rate': 0.007767231050445996, 'num_leaves': 57, 'reg_alpha': 0.04097838404683824, 'reg_lambda': 0.05851461977021186, 'colsample_bytree': 0.5888225655112317, 'subsample': 0.7822836535337736} because of the following error: TypeError(\"LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\").\nTraceback (most recent call last):\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"runfile.py\", line 261, in objective_lgb\n    model.fit(\nTypeError: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\n[W 2026-01-11 07:09:34,938] Trial 0 failed with value None.\nTraceback (most recent call last):\n  File \"C:\\Users\\opnin\\OneDrive\\Desktop\\aide\\aideml\\aide\\interpreter.py\", line 147, in _run_session\n    exec(compile(code, self.agent_file_name, \"exec\"), global_scope)\n  File \"runfile.py\", line 375, in <module>\n    study_lgb.optimize(objective_lgb, n_trials=25)\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\study.py\", line 490, in optimize\n    _optimize(\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 67, in _optimize\n    _optimize_sequential(\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 164, in _optimize_sequential\n    frozen_trial_id = _run_trial(study, func, catch)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 262, in _run_trial\n    raise func_err\n  File \"C:\\Users\\opnin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"runfile.py\", line 261, in objective_lgb\n    model.fit(\nTypeError: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: 5 seconds seconds (time limit is an hour)."], "analysis": ["The code implements a stacking ensemble of LightGBM, XGBoost, CatBoost, and Ridge regression models for house price prediction. It performs extensive feature engineering, missing value imputation, and data transformation. The final RMSE on the out-of-fold predictions is 0.108.", "The code failed because it tried to calculate the median of a column containing strings. Specifically, the error occurred when calling all_data.fillna(all_data.median()). The fix is to only fill numerical columns with the median.", "XGBoost cross-validation failed because early stopping was enabled without any validation data. Remove early_stopping_rounds from the XGBoost model definition.", "The code failed because the input matrix X for LassoCV contains NaN values. Need to impute or drop NaN values before using LassoCV for feature selection.", "The LGBMRegressor.fit() function received an unexpected keyword argument 'early_stopping_rounds'. This is because the early stopping functionality is now handled via callbacks. The code should be updated to use the callbacks argument instead."], "exp_name": "2-quick-starfish-from-hell", "metrics": [0, 0, 0, 0, 0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
