"""
The journal is the core datastructure in AIDE that contains:
- the generated code samples
- information how code samples relate to each other (the tree structure)
- code execution results
- evaluation information such as metrics
- plan review artifacts for human-in-the-loop workflow
"""

import time
import uuid
from dataclasses import dataclass, field
from typing import Literal, Optional

from dataclasses_json import DataClassJsonMixin
from .interpreter import ExecutionResult
from .utils.metric import MetricValue
from .utils.response import trim_long_string


@dataclass
class PlanArtifact(DataClassJsonMixin):
    """
    Stores a plan that has been reviewed by a human in the dual-model workflow.
    
    Tracks the original plan from the Planner, any modifications made during
    human review, and metadata about the review process.
    """
    original_plan: str              # Plan as generated by the Planner model
    approved_plan: str              # Plan after human review/modification
    review_timestamp: float         # When the plan was reviewed
    reviewer_comments: Optional[str] = None  # Optional comments from reviewer
    was_modified: bool = False      # Whether the plan was modified during review
    
    def __post_init__(self):
        """Set review timestamp if not provided."""
        if self.review_timestamp is None:
            self.review_timestamp = time.time()
    
    @property
    def modification_summary(self) -> str:
        """Get a brief summary of the review status."""
        if self.was_modified:
            return "Modified during review"
        return "Approved as-is"


@dataclass(eq=False)
class Node(DataClassJsonMixin):
    """
    A single node in the solution tree. 
    
    Contains code, execution results, evaluation information, and plan review artifacts
    for the dual-model architecture.
    """

    # ---- code & plan ----
    code: str
    plan: str = field(default=None, kw_only=True)  # type: ignore

    # ---- general attrs ----
    step: int = field(default=None, kw_only=True)  # type: ignore
    id: str = field(default_factory=lambda: uuid.uuid4().hex, kw_only=True)
    ctime: float = field(default_factory=lambda: time.time(), kw_only=True)
    parent: Optional["Node"] = field(default=None, kw_only=True)
    children: set["Node"] = field(default_factory=set, kw_only=True)

    # ---- execution info ----
    _term_out: list[str] = field(default=None, kw_only=True)  # type: ignore
    exec_time: float = field(default=None, kw_only=True)  # type: ignore
    exc_type: str | None = field(default=None, kw_only=True)
    exc_info: dict | None = field(default=None, kw_only=True)
    exc_stack: list[tuple] | None = field(default=None, kw_only=True)

    # ---- evaluation ----
    # post-execution result analysis (findings/feedback)
    analysis: str = field(default=None, kw_only=True)  # type: ignore
    metric: MetricValue = field(default=None, kw_only=True)  # type: ignore
    # whether the agent decided that the code is buggy
    # -> always True if exc_type is not None or no valid metric
    is_buggy: bool = field(default=None, kw_only=True)  # type: ignore

    # ---- dual-model / human review artifacts ----
    plan_artifact: Optional[PlanArtifact] = field(default=None, kw_only=True)
    was_human_reviewed: bool = field(default=False, kw_only=True)
    
    # ---- model tracking ----
    planner_model: Optional[str] = field(default=None, kw_only=True)  # Model used for plan
    coder_model: Optional[str] = field(default=None, kw_only=True)    # Model used for code

    def __post_init__(self) -> None:
        if self.parent is not None:
            self.parent.children.add(self)

    @property
    def stage_name(self) -> Literal["draft", "debug", "improve"]:
        """
        Return the stage of the node:
        - "draft" if the node is an initial solution draft
        - "debug" if the node is the result of a debugging step
        - "improve" if the node is the result of an improvement step
        """
        if self.parent is None:
            return "draft"
        return "debug" if self.parent.is_buggy else "improve"

    def absorb_exec_result(self, exec_result: ExecutionResult):
        """Absorb the result of executing the code from this node."""
        self._term_out = exec_result.term_out
        self.exec_time = exec_result.exec_time
        self.exc_type = exec_result.exc_type
        self.exc_info = exec_result.exc_info
        self.exc_stack = exec_result.exc_stack

    @property
    def term_out(self) -> str:
        """Get the terminal output of the code execution (after truncating it)."""
        return trim_long_string("".join(self._term_out))

    @property
    def is_leaf(self) -> bool:
        """Check if the node is a leaf node in the solution tree."""
        return not self.children

    def __eq__(self, other):
        return isinstance(other, Node) and self.id == other.id

    def __hash__(self):
        return hash(self.id)

    @property
    def debug_depth(self) -> int:
        """
        Length of the current debug path
        - 0 if the node is not a debug node (parent is not buggy)
        - 1 if the parent is buggy but the skip parent isn't
        - n if there were n consecutive debugging steps
        """
        if self.stage_name != "debug":
            return 0
        return self.parent.debug_depth + 1  # type: ignore
    
    @property
    def review_status(self) -> str:
        """Get the human review status for this node."""
        if not self.was_human_reviewed:
            return "auto"
        if self.plan_artifact is None:
            return "reviewed"
        return self.plan_artifact.modification_summary


@dataclass
class InteractiveSession(DataClassJsonMixin):
    """
    A collection of nodes for an interaction session
    (when the agent interacts with a Jupyter notebook-like interface).
    """

    nodes: list[Node] = field(default_factory=list)
    completed: bool = False

    def append(self, node: Node) -> None:
        node.step = len(self.nodes)
        self.nodes.append(node)

    def generate_nb_trace(self, include_prompt, comment_headers=True) -> str:
        """Generate a trace of the interactive session in IPython format."""
        trace = []
        header_prefix = "## " if comment_headers else ""
        for n in self.nodes:
            trace.append(f"\n{header_prefix}In [{n.step + 1}]:\n")
            trace.append(n.code)
            trace.append(f"\n{header_prefix}Out [{n.step + 1}]:\n")
            trace.append(n.term_out)

        if include_prompt and self.nodes:
            trace.append(f"\n{header_prefix}In [{self.nodes[-1].step + 2}]:\n")

        return "\n".join(trace).strip()


@dataclass
class Journal(DataClassJsonMixin):
    """A collection of nodes representing the solution tree."""

    nodes: list[Node] = field(default_factory=list)
    # eda: InteractiveSession = field(default_factory=lambda: InteractiveSession())

    def __getitem__(self, idx: int) -> Node:
        return self.nodes[idx]

    def __len__(self) -> int:
        """Return the number of nodes in the journal."""
        return len(self.nodes)

    def append(self, node: Node) -> None:
        """Append a new node to the journal."""
        node.step = len(self.nodes)
        self.nodes.append(node)

    @property
    def draft_nodes(self) -> list[Node]:
        """Return a list of nodes representing initial coding drafts."""
        return [n for n in self.nodes if n.parent is None]

    @property
    def buggy_nodes(self) -> list[Node]:
        """Return a list of nodes that are considered buggy by the agent."""
        return [n for n in self.nodes if n.is_buggy]

    @property
    def good_nodes(self) -> list[Node]:
        """Return a list of nodes that are not considered buggy by the agent."""
        return [n for n in self.nodes if not n.is_buggy]
    
    @property
    def human_reviewed_nodes(self) -> list[Node]:
        """Return a list of nodes that were reviewed by a human."""
        return [n for n in self.nodes if n.was_human_reviewed]
    
    @property
    def modified_plan_nodes(self) -> list[Node]:
        """Return a list of nodes whose plans were modified during review."""
        return [
            n for n in self.nodes 
            if n.plan_artifact is not None and n.plan_artifact.was_modified
        ]

    def get_metric_history(self) -> list[MetricValue]:
        """Return a list of all metric values in the journal."""
        return [n.metric for n in self.nodes]

    def get_best_node(self, only_good=True) -> None | Node:
        """Return the best solution found so far (node with the highest validation metric)."""
        if only_good:
            nodes = self.good_nodes
            if not nodes:
                return None
        else:
            nodes = self.nodes
        return max(nodes, key=lambda n: n.metric)

    def generate_summary(self, include_code: bool = False) -> str:
        """Generate a summary of the journal for the agent."""
        summary = []
        for n in self.good_nodes:
            summary_part = f"Design: {n.plan}\n"
            if include_code:
                summary_part += f"Code: {n.code}\n"
            summary_part += f"Results: {n.analysis}\n"
            summary_part += f"Validation Metric: {n.metric.value}\n"
            summary.append(summary_part)
        return "\n-------------------------------\n".join(summary)
    
    def generate_review_report(self) -> str:
        """Generate a report of all human reviews performed during the experiment."""
        reviewed = self.human_reviewed_nodes
        if not reviewed:
            return "No human reviews performed during this experiment."
        
        report = ["# Human Review Report\n"]
        report.append(f"Total nodes: {len(self.nodes)}")
        report.append(f"Human reviewed: {len(reviewed)}")
        report.append(f"Plans modified: {len(self.modified_plan_nodes)}")
        report.append("\n## Review Details\n")
        
        for node in reviewed:
            report.append(f"### Step {node.step}: {node.stage_name.upper()}")
            report.append(f"- Review status: {node.review_status}")
            if node.plan_artifact and node.plan_artifact.reviewer_comments:
                report.append(f"- Comments: {node.plan_artifact.reviewer_comments}")
            report.append("")
        
        return "\n".join(report)
